{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seba485/Deep-Learning-project/blob/DEV_Hyun/alzheimer_mri_model_tensorflow_2_3_data_loading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5XDfcWdkxrt"
      },
      "source": [
        "# Methods: project work 2023\n",
        "\n",
        "- Every new markdown cell has  'NEW:'      written before\n",
        "\n",
        "- Original markdown cells not useful are commented out \"( <!-- [.]-->)\"\n",
        "\n",
        "- Put your dataset path  containing the dataset, after \" dataset_path \" variable.\n",
        "\n",
        "- Some old code not useful is commented (#)\n",
        "\n",
        "Be aware of all packages needed to be installed to import functions and libraries of the first cell\n",
        "# Introduction + Set-up\n",
        "\n",
        "<!--Machine learning has a phenomenal range of application in the health scienc-es. This tutorial will go over the complete pipeline to build a model that can determine the dementia level of an Alzheimer's patient from their MRI image. This model achieves an a high ROC AUC score.\n",
        "\n",
        "This tutorial highlights the ease of building a CNN using `tf.keras`. Addition-ally, TensorFlow 2.3 has new features, including easy data loading utilities that were previously not available in TensorFlow 2.2. We'll be seeing how easy data loading is with these additional features.-->\n",
        "\n",
        "We'll be using a GPU accelerator and Google Colab for this NB.\n",
        "\n",
        "# LITERATURE REVIEW\n",
        "Alzheimer's disease is a progressive and irreversible neurological disease caused by degeneration of some nerve cells. AD is the most common form of demen-tia (6th cause of death in the USA, from recent studies [12]), is prevalent in older age groups, with a higher percentage in those aged 85 or older [10]. The number of AD cases is expected to rise globally, making it a significant public health concern.\n",
        "Its main syntomps are loss of memory and other cognitive functions impairment that worsen patients life conditions and make them completely dependent for basic daily functions, eventually leading to death. Its early detection is crucial for slowing disease progression and a proper diagnosis of the patient requires classification of different dementia stages [5], as this parametre is informative about cells degeneration, disease seriousness and burden, and possi-bility of intervenctions.\n",
        "\n",
        "MRI is a non-invasive and not costly imaging technique widely used for AD diagnosis, also thanks to widespread avaliability of MRI images and scanners: it allows to assess integrity of brain regions and provides evidence of hippocampus and cortex shrink occurring in AD patients [9].\n",
        "In recent years, many Machine Learning algorithms were devolped for automatic AD detection. Among these, deep learning models seem preferable since they perform automatic feature extraction  from raw data and do not require handcrafted features of brain structures, labourious and time consuming. Among these, convolutional neural networks  are well-suited for analysing image data.  Moreover, according to some studies, transfer learning were most successfull due to the limited size of avaliable neuroimage datasets [9].\n",
        "Many works were presented in the last years that reached excellent performances in both simple detecting ([3] [4]) and classifying AD stage ([5],[2]) through deep-learning models. Major issues are limited amount of medical data that might  cause overfitting, the need for high computational resources for training these huge models and very unbalanced classes leading to misleading performances.\n",
        "\n",
        "# ABSTRACT\n",
        "\n",
        "*Description of the original project*:\n",
        "\n",
        "This project was about determining the dementia level of Alzheimers patients from their MRI images. The original code from kaggle provided a dataset organized in 4 classes corresponding to the dementia level: non demented, very mild demented, mild demented and moderate demented. First of all the dataset was split between test and training with a ratio of 20% and 80% (The used Deep Learning framework was Tensorflow). Then a CNN was trained exploiting the training set in order to classify the images belonging to the test set. After 100 epochs of training, both training and validation loss decreased over these; the model was tested with AUC metric on the test set and the results were optimal (AUC: 0.8360 - Loss: 1.5338)\n",
        "\n",
        "*Limitations of the original project*:\n",
        "\n",
        "Even if the values of the metrics suggested a good performance of the model, on the old dataset , these values looked too suspicious for us. We checked the model and everything seemed to be ok, so we took a close look at the dataset and we noticed that there was something strange. Additional information retrieved from past studies on the same dataset suggested the images in the train and in the test set belonged to the same patients, they were just different slices of the same MRI acquisition.\n",
        "\n",
        "Another strong limitation of the original project is the limited evaluation of results: no confusion matrices were created, only loss and AUC on test set were computed as metrics.\n",
        "\n",
        "The given CNN applied to the original dataset, learns to detect patients but not to recognise the dementia level and classify correctly images (class-specific recall for three classes very low):  the model does not fulfill our expectations but just detects patients (all demented classes): we questionned whether this was an overfitting issue.\n",
        "\n",
        "*Objective*:\n",
        "\n",
        "Solve the problem of the dataset; Reduce the risk of overfitting over new dataset; Exploit additional metrics to evaluate the model; implement new methods to improve performances.\n",
        "\n",
        "*Methods*:\n",
        "\n",
        "- Dataset:  it was quite impossible to tidy up the given dataset, so we choose to change the dataset using the one reported in the following link: https://www.kaggle.com/datasets/sachinkumar413/alzheimer-mri-dataset/data.\n",
        "\n",
        "- Hyperparametres tuning: in order to reduce the overfitting chances we tuned the model working on its hyperparameters such as the learning rate decay, L2- regularization weights and drop-out values. Furthermore, we performed data augmentation on the training set to balance the dataset.\n",
        "- Evaluation of the model: the trained model was evaluated on AUC,Loss and Accuracy, and a confusion matrix was built to better understand how the model performed. Also, recall, precision and balanced accuracy were considered.\n",
        "- Try to modify the model in order to fit better the new dataset.\n",
        "- Transfer learning: we used transfer learning methods to see if the usage of a more complex and pre-trained neural network could lead to better performances. We chose Denset121, Resnet50, Resnet101 and Vgg19. We froze the backbone, resized the input layer, and added in order a averagepooling layer, a dense layer activated by a Relu and the output layer in SoftMax activation.\n",
        "\n",
        "*Results*:\n",
        "\n",
        "Our model, with the original architecture proposed and without acquiring transfered knowledge, is not able to do classify the disease stage.\n",
        "\n",
        "The original model with the new dataset struggles to identify moderate demented and mild demented also using augmented data and after hyperarametres tuning. Moderate demented class is the most critical, while Mild Demented shows very little improvement in sensitivities with these precautions. We reach the conclusion that the main problem is the model architecture that is unsuitable.\n",
        "\n",
        "Transfer learning techniques on different models showed higher capabilities in fulfilling the purpose of the project.\n",
        "\n",
        "\n",
        "\n",
        "# NEW DATASET: [Alzheimer MRI Preprocessed Da-taset](https://www.kaggle.com/datasets/sachinkumar413/alzheimer-mri-dataset/)\n",
        "\n",
        "The Dataset consists of preprocessed MRI (Magnetic Resonance Imaging) images collected from several websites/hospitals/public repositories. There are 6400 MRI images in total and all images are resized into 128 x 128 pixels. The Dataset comprises four classes of images:\n",
        " - Class - 1: Mild Demented (896 images)\n",
        " - Class - 2: Moderate Demented (64 images)\n",
        " - Class - 3: Non Demented (3200 images)\n",
        " - Class - 4: Very Mild Demented (2240 images)\n",
        "\n",
        "Data, despite being collected from various sources, have been already preprocessed and harmonised (All images resized into 128 x 128 pixels and pixels val-ues in [0,255] scale, so we didn't have to deal with data heterogeneity). Data  are already 2 dimensional images ready to be fed to the network, so we didn't have to deal with data heterogeneity and data harmoni-sation. This code is suitable for any dataset of already preprocessed MRI.\n",
        "\n",
        "To enhance adaptability, our code will be designed to handle analogous datasets. For instance, the batch size will be dynamically defined based on the number of files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfsrpsILn4WP"
      },
      "source": [
        "\n",
        "NEW:\n",
        "The original code imported all necessary packages for previous version\n",
        "We import additional packages and removed redundant or unused packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-32z2lDZkxr0",
        "outputId": "0c33798c-55a8-4fa7-88b1-41ec137e4c2a"
      },
      "outputs": [],
      "source": [
        "##### import packages #####\n",
        "import os\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import shutil\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import random\n",
        "print(tf.__version__)\n",
        "\n",
        "# import kaggledatasets\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "try:\n",
        "  from scikeras.wrappers import KerasClassifier\n",
        "except:\n",
        "  #pip install scikeras\n",
        "  from scikeras.wrappers import KerasClassifier\n",
        "from keras.models import load_model\n",
        "import itertools\n",
        "\n",
        "#For transfer learning\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.applications  import DenseNet121,ResNet101,ResNet50,VGG19\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "import visualkeras\n",
        "from PIL import ImageFont"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoBdO8pQn4WS"
      },
      "source": [
        "NEW:\n",
        "\n",
        "The original code sets up a TensorFlow TPU (Tensor Processing Unit) environment and shows the number of replicas in sync. The `strategy.num_replicas_in_sync` attribute, namely the number of replicas in sync, is the number of devices (TPU cores) that are used in parallel during training. Utilizing TPU enables the distribution of the training process across multiple TPU cores, enhancing computational power significantly.\n",
        "\n",
        "Furthermore, the code to check whether the execution environment is Google Colab environment or a local environment is included.\n",
        "In addition to TPU utilization, we have considered the option of running the code on GPU (Graphics Processig Unit) environment. Therefore, the code includes checks for the availablity of GPU usage and the determination of the number of GPUs accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25t2mLNLn4WT",
        "outputId": "34e50e1d-3c1c-448a-8285-df2d93d429cb"
      },
      "outputs": [],
      "source": [
        "##### set-up gpu/tpu #####\n",
        "# check & mount in case of using colab\n",
        "'''\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    print('running the notebook in colab')\n",
        "except:\n",
        "    pass\n",
        "'''\n",
        "\n",
        "# check if tpu is available\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Device:', tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except:\n",
        "    print('No TPU found')\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
        "\n",
        "# check if gpu is available\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(gpus)\n",
        "if gpus:\n",
        "    print(f\"Found {len(gpus)} GPU\")\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJVYG-Iqkxr7"
      },
      "source": [
        "It's always a good idea to set constant variables instead of hard coding numbers into the code. It saves time later when we want to change certain variables.\n",
        "\n",
        "NEW:\n",
        "\n",
        "The original code defines variables for the transformation within the pipeline input data mechanism. By using `AUTOTUNE`, tf.data API will automatically tune the performance of data loading operation, which adapts the level of parallelism in executing operations based on available computational resources. This can be particularly useful when working with distributed computing environments like TPUs or GPUs, where the performance characteristics may vary. This helps to make the input pipeline more flexible, faster, highly performant and adaptive to different hardware configurations.\n",
        "\n",
        "The original code also defines variables for epochs, image size, and batch size.\n",
        "\n",
        "- STRENGTH ORIGINAL CODE: performance improvement of input data pipeline Tensorflow\n",
        "- WEAKNESS ORIGINAL CODE: by setting a fixed batch size, it is not dynamically adjusted based on the dataset size\n",
        "\n",
        "We set image size according to the size of the dataset.\n",
        "\n",
        "We calculate Batch Size later on the code and define number of epochs later, before fitting the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "B9A6U6Fokxr9"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "# BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "IMAGE_SIZE = [128, 128]  #[176,208]\n",
        "# EPOCHS = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9KhC_7qkxr-"
      },
      "source": [
        "# Data Loading\n",
        "\n",
        "<!--We'll be using a [Kaggle Alzheimer's dataset](https://www.kaggle.com/tourist55/alzheimers-dataset-4-class-of-images) for our tutorial. -->\n",
        "\n",
        "NEW:\n",
        "\n",
        "We changed dataset the train and the test set contain different sections of same subjects in the original dataset. (some volumes and some single slices ?)\n",
        "Upon reviewing feedback on Kaggle, we observed concerns regarding the reliability of the original dataset.\n",
        "In response, we selected a new dataset sourced from Kaggle, maintaining the same categories and has more citations related, which is expected to be more reliable.\n",
        "\n",
        "To load the images and save it as `tf.data.Dataset` Object, the original code utilized `tf.keras.preprocessing.image_dataset_from_directory` function. It is a new preprocessing function that can easily load in images for a directory. In order for this function to work, the data has to be structured in a file directory format.\n",
        "```\n",
        "main_directory/\n",
        "    class1/\n",
        "        class1_images\n",
        "    class2/\n",
        "        class2_images\n",
        "```\n",
        "If we input the `main_directory` into the `tf.keras` function, it will figure out the rest.\n",
        "\n",
        "<!--In our case, the `train` directory is our main directory.\n",
        "\n",
        "We are also specifying a 80:20 split for our training and validation datasets. To learn more about the importance of having a validation split, check out this [lesson](https://developers.google.com/machine-learning/crash-course/validation/another-partition) from Google's Machine Learning Crash Course.-->\n",
        "\n",
        "NEW:\n",
        "\n",
        "In the previews dataset, the train and test files are located in separate folders which allowed easier usage of this function. However, the new dataset does not have separated folders for each train and test set. <!--In order to keep `tf.keras` preprocessing function as function for input data ( this takes you from one images directory on a disk to a n tf.data.Dataset in just two codes lines),--> Therefore, we create train and test folders respecting the proportion among classes.Folders are created (and files are copied) only if not present (and if files not already copied) to avoid running the same code multiple times.\n",
        "\n",
        "We set dataset splitting ratio as train:val:test=0.8\\*0.8:0.8\\*0.2:0.2, resulting 4095 images for training, 1023 images for validation, and 1282 images for testing.\n",
        "\n",
        "We choose batch size customised to our dataset which is proportional to number of files in folders to make the code more adaptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUCetvgekxsA",
        "outputId": "b7ce1859-a0f7-4475-bc1b-089d15239946"
      },
      "outputs": [],
      "source": [
        "##### split and save dataset into train/val and test data #####\n",
        "\n",
        "# specify local dataset path\n",
        "\n",
        "# WRITE YOUR OWN PATH CONTAINING DATASET\n",
        "dataset_path = 'Dataset'\n",
        "\n",
        "train_path = dataset_path+'/train' #folder doesnâ€™t exist yet\n",
        "test_path= dataset_path+'/test'\n",
        "test_size = 0.2\n",
        "val_size = 0.2\n",
        "classes = os.listdir(dataset_path)\n",
        "if 'test' in classes:\n",
        "   classes.remove('test')\n",
        "if 'train' in classes:\n",
        "  classes.remove('train')\n",
        "# if train / test folders not created (first time running code), 'test' and  'train' not present\n",
        "\n",
        "print(classes)\n",
        "\n",
        "if os.path.isdir(train_path):\n",
        "    print('Dataset has already been splited and saved into train/val and test data.')\n",
        "else:\n",
        "    os.makedirs(train_path, exist_ok=True)\n",
        "    os.makedirs(test_path, exist_ok=True)\n",
        "\n",
        "    for class_name in classes:\n",
        "        print(class_name)\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        train_class_path = os.path.join(train_path, class_name)\n",
        "        test_class_path = os.path.join(test_path, class_name)\n",
        "\n",
        "        # Get the list of files in the class\n",
        "        files = os.listdir(class_path)\n",
        "        os.makedirs(train_class_path, exist_ok=True)\n",
        "        os.makedirs(test_class_path, exist_ok=True)\n",
        "\n",
        "        # Divide the class dataset into training and test sets\n",
        "        train_files, test_files = train_test_split(files, test_size=test_size)\n",
        "\n",
        "        # Save the training files to the training folder\n",
        "        for file in train_files:\n",
        "            src_path = os.path.join(class_path, file)\n",
        "            dst_path = os.path.join(train_path, class_name, file)\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "        # Save the test files to the test folder\n",
        "        for file in test_files:\n",
        "            src_path = os.path.join(class_path, file)\n",
        "            dst_path = os.path.join(test_path, class_name, file)\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "\n",
        "##### split train/val into train and val data #####\n",
        "# set batch size: 1) batch_size=40, 2) take 2% of the train+val set as the batch size\n",
        "customized_batchsize = True # True if we want customized batch size over the dataset size\n",
        "if customized_batchsize:\n",
        "    trainval_cnt = sum([len(files) for _, _, files in os.walk(train_path)])\n",
        "    test_cnt = sum([len(files) for _, _, files in os.walk(test_path)]) # might use this in case of using different batch size for test set\n",
        "    BATCH_SIZE = round(trainval_cnt * 0.02)\n",
        "else:\n",
        "    BATCH_SIZE = 40  # original batch size\n",
        "print(\"Batch Size for train, validation and test set:\", BATCH_SIZE)\n",
        "\n",
        "# split dataset and save into tf Dataset object\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_path,\n",
        "    validation_split=val_size,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    color_mode='grayscale',  ## gray scale loading (1d tensor depth)\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_path,\n",
        "    validation_split=val_size,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    color_mode='grayscale',\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Import of the original dataset\n",
        "train_ds_OLD = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"Alzheimer_s Dataset/train\",\n",
        "    validation_split=val_size,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    color_mode='grayscale',  ## gray scale loading (1d tensor depth)\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "val_ds_OLD = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"Alzheimer_s Dataset/train\",\n",
        "    validation_split=val_size,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    color_mode='grayscale',\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO9jZwBUkxsC"
      },
      "source": [
        "<!--We'll be renaming the class names and specifying the number of classes. In this case, we have 4 classes of dementia.-->\n",
        "\n",
        "NEW:\n",
        "\n",
        "The original code renames the class names for training and validation set and sets the variable for number of classes.\n",
        "We does not raname classes as it is inconsistent with the folder names. In our code, we comment those lines out so classes names can be left to be folders (previously created) names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AkdFo1fkxsD"
      },
      "outputs": [],
      "source": [
        "#class_names = ['MildDementia', 'ModerateDementia', 'NonDementia', 'VeryMildDementia']\n",
        "#train_ds.class_names = class_names\n",
        "# val_ds.class_names = class_names\n",
        "NUM_CLASSES = len(classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb1AS9I5kxsD"
      },
      "source": [
        "# Visualize the data\n",
        "\n",
        "Now that our data has been easily loaded in, the next step is to visualize our images. This helps us understand what is being used as an input for our model. It also serves as a check to see if our images have been loaded in correctly.\n",
        "\n",
        "NEW:\n",
        "\n",
        "Data leakage test by 2 methods: file name, gray values hash.\n",
        "\n",
        "The original code explores the data by visualizing a grid of nine images along with their corresponding labels. To display images using the `matplotlib.imshow` function, `astype(\"uint8\")` is used to convert the pixel values to 8-bit unsigned integers.\n",
        "\n",
        "- STRENGTH ORIGINAL CODE: look at some images to check they were uploaded in the right way and to have a preliminary look at data.\n",
        "- WEAKNESS ORIGINAL CODE: just plot some images from the first batch, does not distinguish graphically classes. Does not look for unbalanced classes in the training set by doing some exploratory plot (for example histograms).\n",
        "\n",
        "Thus, we display 5 samples per each class. To do this, build a dictionary where keys are folder names (-> the 4 classes) and values are the filenames for the files in each folder (for both test and train). We then show images by classes to see if differences are visible by hand.\n",
        "Additionally, we check the number of samples within each class in both train and test set usig this dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUnBCDyB7SRt",
        "outputId": "286caabd-122d-44be-db42-7ecf6b82c2e8"
      },
      "outputs": [],
      "source": [
        "#Data leakage test,this function checks if there are file in train/test with the same name\n",
        "def check_data_leakage_name(train_folder, test_folder):\n",
        "    # Get the class list\n",
        "    classes = [d for d in os.listdir(train_folder) if os.path.isdir(os.path.join(train_folder, d))]\n",
        "    # Start the test, if there is file with the same in both folders\n",
        "    leakage_detected = False\n",
        "    for cls in classes:\n",
        "        train_cls_folder = os.path.join(train_folder, cls)\n",
        "        test_cls_folder = os.path.join(test_folder, cls)\n",
        "        common_images = set(os.listdir(train_cls_folder)).intersection(set(os.listdir(test_cls_folder)))\n",
        "        if common_images:\n",
        "            print(f\"Data leakage detected in class {cls}. Common images: {common_images}\")\n",
        "            leakage_detected = True\n",
        "\n",
        "\n",
        "    if not leakage_detected:\n",
        "        print(\"No data leakage detected.\")\n",
        "\n",
        "# specify the test and training folders\n",
        "train_folder = \"Dataset/train\"\n",
        "test_folder = \"Dataset/test\"\n",
        "\n",
        "# verify data leakage\n",
        "check_data_leakage_name(train_folder, test_folder)\n",
        "#there are not file with the same name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KtpWFH670b5",
        "outputId": "d3d3cc80-4636-4ef4-f086-c250bf5647a8"
      },
      "outputs": [],
      "source": [
        "#let's check the flawed one\n",
        "\n",
        "flaw_train_folder='Alzheimer_s Dataset/train'\n",
        "flaw_test_folder='Alzheimer_s Dataset/test'\n",
        "check_data_leakage_name(flaw_train_folder, flaw_test_folder)\n",
        "#there are not file with the same name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26P9OQmf_gqx"
      },
      "outputs": [],
      "source": [
        "def hash_image(image_path):\n",
        "    # Calcola l'hash dell'immagine\n",
        "    image = Image.open(image_path)\n",
        "    image_array = np.array(image)\n",
        "    return hash(image_array.tobytes())\n",
        "\n",
        "def check_data_leakage(train_folder, test_folder):\n",
        "    classes = ['Mild_Demented','Moderate_Demented','Non_Demented','Very_Mild_Demented']\n",
        "    # Verifica la presenza di immagini comuni basate sugli hash\n",
        "    leakage_detected = False\n",
        "    for cls in classes:\n",
        "        train_cls_folder = os.path.join(train_folder, cls)\n",
        "        test_cls_folder = os.path.join(test_folder, cls)\n",
        "\n",
        "        train_hashes = set(hash_image(os.path.join(train_cls_folder, file_name)) for file_name in os.listdir(train_cls_folder))\n",
        "        test_hashes = set(hash_image(os.path.join(test_cls_folder, file_name)) for file_name in os.listdir(test_cls_folder))\n",
        "\n",
        "        common_hashes = train_hashes.intersection(test_hashes)\n",
        "        if common_hashes:\n",
        "            print(f\"Data leakage detected in class {cls}. Common image hashes: {common_hashes}\")\n",
        "            leakage_detected = True\n",
        "    if not leakage_detected:\n",
        "        print(\"No data leakage detected\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui7nHgiv_1bC",
        "outputId": "e0a5f86e-6914-4484-9f5a-5751c9ed47c0"
      },
      "outputs": [],
      "source": [
        "check_data_leakage(flaw_train_folder,flaw_test_folder )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kdCVPjn8kxsE",
        "outputId": "d9057b89-8785-4924-fb80-c8182141702e"
      },
      "outputs": [],
      "source": [
        "##### check the number of samples within each class in train set #####\n",
        "# create a dictionary where:\n",
        "\n",
        "# - keys are folder names (-> the 4 classes)\n",
        "# - values are the filenames for the files in each folder : training set\n",
        "imgs = {c: glob(os.path.join(train_path, c, \"*\")) for c in classes}\n",
        "for key in imgs.keys():\n",
        "   print(\"{} class has {} samples in train set.\".format(key,len(imgs[key])))\n",
        "\n",
        "# - keys are folder names (-> the 4 classes)\n",
        "# - values are the filenames for the files in each folder :  test set\n",
        "imgs = {c: glob(os.path.join(test_path, c, \"*\")) for c in classes}\n",
        "for key in imgs.keys():\n",
        "   print(\"{} class has {} samples in test set.\".format(key,len(imgs[key])))\n",
        "\n",
        "\n",
        "# PROPORTION IS RESPECTED\n",
        "\n",
        "##### plot 5 samples per class #####\n",
        "# CHANGE--> PLOT SOME (5) IMAGES PER CLASS ! TO SEE BETTER DIFFERENCES\n",
        "nrows = 5\n",
        "fig, axs = plt.subplots(nrows=nrows, ncols=len(classes), figsize=(20, 20))\n",
        "\n",
        "for i, (c, fnames) in enumerate(imgs.items()): # i is the count, (c, fnames) key and value\n",
        "    axs[0,i].set_title(c)\n",
        "    for n in range(nrows):\n",
        "        fname = (fnames[n]).replace('\\\\','/')\n",
        "        im = Image.open(fname)\n",
        "        # im = im.convert('L') # image is already saved in 'L' mode\n",
        "        axs[n,i].imshow(im, cmap='gray')\n",
        "for ax in axs.flatten():\n",
        "    ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N10z2awhn4WX"
      },
      "source": [
        "# NEW: A BETTER LOOK AT DATA !\n",
        "NEW: We have a deeper esploratory data analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzW0BY9kn4WY"
      },
      "source": [
        "NEW: We define a function to extract our images and labels from Dataset format required by keras. We chose to keep this format for the Dataset as it is required as model input. We basically extend  a list of data observations (all_images) and a list of data targets (all_labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC2J7BZBsSF3"
      },
      "outputs": [],
      "source": [
        "##### extract numpy arrays of images and labels from tensorflow Dataset Object #####\n",
        "#train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "#val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "def images_labels_fromDataset(dataset):\n",
        "    dataset_it = dataset.as_numpy_iterator() # Numpy iterator, the object in input is not iterable\n",
        "    all_labels=[]; all_images=[]\n",
        "    for batch_images, batch_labels in dataset_it:\n",
        "        all_labels.extend(batch_labels)\n",
        "        all_images.extend(batch_images)\n",
        "    return all_images, all_labels\n",
        "\n",
        "tensors_train, labels_train = images_labels_fromDataset(train_ds)\n",
        "tensors_val, labels_val = images_labels_fromDataset(val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhVbgaWFn4WY"
      },
      "outputs": [],
      "source": [
        "print(len(labels_train))\n",
        "print(len(tensors_train))\n",
        "print(tensors_train[0].shape) # to check it is correct\n",
        "print(BATCH_SIZE)\n",
        "\n",
        "# everything is correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiKIfvSLn4WZ"
      },
      "source": [
        "NEW: we look better at a single observation = a single tensor. We check its shape, verify if it is 2D (channels are three but identicals), and compute maximum, minimum, mean. We find that images are scaled in the range [0, 255]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXYXF0s_n4WZ"
      },
      "outputs": [],
      "source": [
        "# have a close look at the format of the tensors data and at their values\n",
        "s=tensors_train[0].shape  # (128,128,1)\n",
        "\n",
        "example=tensors_train[0]   #3darray (128,128,1); all values in 3rd dimension are the same --> example[i,j,1]=example[i,j,2]=example[i,j,3]\n",
        "\n",
        "#check=np.count_nonzero(example[:,:,1]!=example[:,:,2]) #0\n",
        "#check2=np.count_nonzero(example[:,:,1]!=example[:,:,0]) #0\n",
        "# it's a tensor with three identical channels\n",
        "\n",
        "maxvalue=np.max(example)\n",
        "minvalue=np.min(example)\n",
        "meanvalue=np.mean(example)\n",
        "\n",
        "#image values\n",
        "print('maxvalue: '+str(maxvalue))\n",
        "print('minvalue: '+str(minvalue))\n",
        "print('meanvalue: '+str(meanvalue))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts60KqrNn4WZ"
      },
      "source": [
        "NEW: We check for classes unbalances and plot an histogram of classes distribution. Two subplots: percentage of samples per class and number of samples per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnQnZbGykxsF"
      },
      "outputs": [],
      "source": [
        "##### check class distribution of dataset #####\n",
        "train_labels_name = [classes[idx] for idx in labels_train] #name to the classes\n",
        "val_labels_name = [classes[idx] for idx in labels_val]\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(12,5))\n",
        "\n",
        "vis_ratio = True # if we want to plot the num of samples or the ratio of each class\n",
        "vis_title = 'Percentage of samples per class' if vis_ratio else 'Number of samples per class'\n",
        "df_cnt = pd.concat([pd.Series(train_labels_name).value_counts(normalize=vis_ratio), pd.Series(val_labels_name).value_counts(normalize=vis_ratio)], axis=1)\n",
        "df_cnt.plot(kind='bar', ax=axs[0], title=vis_title, rot=45)\n",
        "axs[0].legend(['train', 'validation'])\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "vis_ratio = False # if we want to plot the num of samples or the ratio of each class\n",
        "vis_title = 'Percentage of samples per class' if vis_ratio else 'Number of samples per class'\n",
        "df_cnt = pd.concat([pd.Series(train_labels_name).value_counts(normalize=vis_ratio), pd.Series(val_labels_name).value_counts(normalize=vis_ratio)], axis=1)\n",
        "df_cnt.plot(kind='bar', ax=axs[1], title=vis_title, rot=45)\n",
        "axs[1].legend(['train', 'validation'])\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "plt.savefig('Dataset distribution.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wec2U2c1n4Wa"
      },
      "outputs": [],
      "source": [
        "print('Training set')\n",
        "print(pd.Series(train_labels_name).value_counts())\n",
        "print('Valid set')\n",
        "print(pd.Series(val_labels_name).value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK-9O9IkkxsG"
      },
      "source": [
        "NEW:\n",
        "\n",
        "From the plot, it is possible to see dataset's imbalance with notably few samples for moderate-dementia. Thus, applying data augmentation becomes essential, and tailored approach for each class is needed in order to prevent underaugmentation of the minority class which might cause potential misleading performance evaluations.\n",
        "\n",
        "In medical research, due to privacy concerns, the access to large data is a big problem [5], especially, the classification of cancer and AD are problematic due to lack of availability of data. It is well known that deep learning models  provide more effective results on more data. The small or imbalanced dataset  can create overfitting problems during training of the model which affects the model efficiency. To overcome this issue, we need more data to enhance the performance in our proposed model. We used the augmentation technique as a resolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkVfI7OCn4Wb"
      },
      "source": [
        "# Deciding a Metric\n",
        "\n",
        "The most conventional metric to use for classification is probably accuracy. Accuracy, however, is unsuitable for imbalanced datasets. We checked if the class is evenly distributed in our training data, and as a result, we found out that our dataset is not balanced, so we cannot use accuracy as our metric.\n",
        "For this tutorial, we will be using AUC(Area Under the ROC Curve). Intuitively, AUC gives a score, with higher scores closer to 1 indicating that the different classes can be distinguishable for the model. A lower scores closer to 0 indicate that the model cannot distinguish different classes. A score of 0.5 indicates that the classification of the images is pretty much random. Learn more about ROC and AUC [here](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc).\n",
        "\n",
        "NEW: the original model decides to use only AUC and will later visualises it as a performance metric.\n",
        "- STRENGHT OF ORIGINAL MODEL: AUC is a stronger indicator than accuracy, as accuracy can be misleading and doesn't account for classes unbalance.\n",
        "- WEAKNESS OF ORIGINAL MODEL: AUC may be less informative in extremely imbalanced datasets (our case), can be high when performances on classes with less samples are worst, and our analysis showed exactly this (see Confusion Matrix of original model). In such cases, precision and recall may provide more insights into the classifier's performance on the minority class.\n",
        "\n",
        "Moreover, AUC-ROC treats false positives and false negatives equally, but in our scenario, the cost of these errors is not the same. In AD classification, a false negative (missing a disease) might be more critical than a false positive (incorrectly predicting a disease)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsjdih0Rn4Wb"
      },
      "source": [
        "# NEW: Class specific data augmentation\n",
        "\n",
        "NEW:\n",
        "\n",
        "To deal with class imbalance issue, we have defined layers for data augmentation. For these, we referred to existing literature ([2], [3], [5], [8]), selecting transformation that do not modify the image size. For example in [5], width shift range, height shift range, and shear range were applied but our model didn't work as image size was altered. While we could have filled this with zero-padding or other filling techniques, we opted against it for the reasons of not having any experties and risking of making a dataset too \"artificial\". As a consequense, we decided to apply only the following transformations: random zoom in (height factor and width factot negative, see [Keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/) for more detail), random rotation, random flip, random  translation, and random brightness.\n",
        "Data augmentation is really adapted for our specific dataset: we could not do this inside the model, even if this would have made our model and code more transferable.\n",
        "\n",
        "Empty spaces are filled according to attribute fill_mode: we first put `fill_mode=\"reflect\"`, but looking carefully at augmented data we realised that augmented images were not suitable for \"representing\" original images (looked like aliasing artifact). So we changed `fill mode=\"constant\"` and it worked better, saving us from processing the image with operators and thresholds. Parameter interpolation is chosen to be \"bilinear\" which is good for general-purpose smoothing while it slightly blurs the image [11].\n",
        "\n",
        "We set always the same seed for code repeatability: this was crucial especially when dealing with input datasets that influence parameter fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZMmFGUxkxsJ"
      },
      "outputs": [],
      "source": [
        "## Layers for data augmentation\n",
        "\n",
        "# randomly zoom in or out on each axis\n",
        "RandomZoom_layer1=(tf.keras.layers.RandomZoom(\n",
        "    height_factor=(-0.1, -0.3),  # lower and upper bound for zooming vertically\n",
        "    width_factor=(-0.1, -0.3), #lower and upper bound for zooming horizontally\n",
        "    fill_mode=\"constant\",\n",
        "    seed=2023\n",
        "))\n",
        "\n",
        "# apply random rotation\n",
        "RandomRotation_layer1=tf.keras.layers.RandomRotation(\n",
        "    (-0.2,0.2),  #lower and upper bound for rotating clockwise and counter-clockwise. (+ counter clock-wise, - clock-wise)\n",
        "    fill_mode='constant',\n",
        "    interpolation='bilinear',\n",
        "    seed=2023)\n",
        "\n",
        "# randomly flips images\n",
        "RandomFlip_layer1=tf.keras.layers.RandomFlip(\n",
        "    mode=\"horizontal_and_vertical\", seed=2023)\n",
        "\n",
        "# randomly translates images\n",
        "RandomTranslation_layer1=tf.keras.layers.RandomTranslation( ## by visual inspection the object still inside the borders\n",
        "    (-0.1,0.1), #height factor: lower and upper bound for shifting vertically\n",
        "     (-0.1,0.1), #width factor: ower and upper bound for shifting horizontally\n",
        "    fill_mode='constant',\n",
        "    interpolation='bilinear',\n",
        "    seed=2023)\n",
        "\n",
        "# randomly adjusts brightness\n",
        "RandomBrightness_layer1=tf.keras.layers.RandomBrightness(\n",
        "    (-0.2,0.2), seed=2023)  #lower bound and upper bound of the brightness adjustment (+ reduced brightness, - increases brightness)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9keIh83vn4Wc"
      },
      "source": [
        "NEW: The following cell is a function that saves the image, or the augmented image, that it receives as first input and puts it inside the folder assigned to \"folderto\".\n",
        "\n",
        "In our first code, during data augmentation (following cell), each image, before and after being augmented, was rescaled. But while fitting the model with augmented data, we observed that validation loss became seriously divergent and the model did not work at all: we realised it was because of the rescaling (from [0, 255] to [0,1]). We concluded that the network probably encountered a Vanishig Gradient Problem due to extremely low inputs:  range   [0, 255], not rescaled range [0,1]  is adequate to let the model learn relationship between data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__4fgc7Bn4Wc"
      },
      "outputs": [],
      "source": [
        "def save_augmented_image(image,i,label,folderto,transf=\"\"):\n",
        "  image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
        "  filename = f\"image_{i}_label_{label}_{transf}.jpg\"  # Use a counter or other identifiers        # Create a unique filename or use some identifier based on your datase\n",
        "  image_path = os.path.join(folderto, filename).replace(\"\\\\\",\"/\")  # Specify the complete path for saving the image\n",
        "  tf.keras.preprocessing.image.save_img(image_path, image_array)          # Save the image\n",
        "\n",
        "'''\n",
        "def rescale_im(im):    #it seems that rescaling the images between 0 and 1 make the model unable to perform the training, validation loss explode\n",
        "  min_value = np.min(im)\n",
        "  max_value = np.max(im)\n",
        "  scaled_im = (im - min_value) / (max_value - min_value)\n",
        "  return scaled_im\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLwpvfYWn4Wd"
      },
      "outputs": [],
      "source": [
        "print(classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LgxE8p1n4Wd"
      },
      "source": [
        "We performed data augmentation on different scales for each class: for example, class 'Non-Demented (label 2)' had the most samples so it was not augmented. Class 'Moderate Demented (label 1)' had 50 times less samples than 'Non-Demented', so 50 augmented images were created for each image. Class 'Very Mild Demented' has a random probability condition because we didn't want to duplicate our dataset, but do \"less then 2 images for image\".\n",
        "\n",
        "For the first 20 iterations, the code also shows (unless the image was not augmented, namely unless image was labeled 2) in a raw the image and its augmented versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSwQNFTskxsL"
      },
      "outputs": [],
      "source": [
        "# DATA AUGMENTATION FOR BALANCING CLASSES\n",
        "\n",
        "#Class - 1: Mild Demented (896 images)\n",
        "#Class - 2: Moderate Demented (64 images)\n",
        "#Class - 3: Non Demented (3200 images)\n",
        "#Class - 4: Very Mild Demented (2240 images)\n",
        "\n",
        "\n",
        "#tensors_train, labels_train=images_labels_fromPrefetch(train_ds)   #1d array of dimension (4095)\n",
        "number_of0=np.count_nonzero(labels_train==0)  #568= Class 1 : Mild Demented--> 3,6 (=4) data aug\n",
        "number_of1=np.count_nonzero(labels_train==1)  #41= Class  2: Moderate Demented (64 images)--> 50 data aug\n",
        "number_of2=np.count_nonzero(labels_train==2)  #2053= Class 3: Non Demented (3200 images)--> 0 data aug\n",
        "number_of3=np.count_nonzero(labels_train==3)  #1433 Class= 4: Very Mild Demented (2240 images)--> 1,43 (=1) data aug\n",
        "\n",
        "\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "folder_augmented_0=\"Augmented_TrainDataset/\"+classes[0]+\"/\"\n",
        "folder_augmented_1=\"Augmented_TrainDataset/\"+classes[1]+\"/\"\n",
        "folder_augmented_2=\"Augmented_TrainDataset/\"+classes[2]+\"/\"\n",
        "folder_augmented_3=\"Augmented_TrainDataset/\"+classes[3]+\"/\"\n",
        "\n",
        "folders_augmented=[folder_augmented_0,folder_augmented_1,folder_augmented_2,folder_augmented_3]\n",
        "for f in folders_augmented:\n",
        " if not os.path.exists(f):\n",
        "        os.makedirs(f)\n",
        " [os.unlink(os.path.join(f, file)) for file in os.listdir(f) if os.path.isfile(os.path.join(f, file))]\n",
        "\n",
        "\n",
        "# Loop that applies  data augmentation to each image according to its label\n",
        "\n",
        "for i in range(len(tensors_train)): #len is 4095, first dimensions\n",
        "    image = tensors_train[i]  #nd array (128,128,3)\n",
        "    label = labels_train[i]\n",
        "\n",
        "    # Apply data augmentation selectively based on the class label\n",
        "    if label == 0: #568  Class 1 : Mild Demented\n",
        "        augmented_image = RandomRotation_layer1(image) #tensor\n",
        "        augmented_image2= RandomTranslation_layer1(image)\n",
        "        augmented_images_conc = tf.keras.layers.Concatenate(axis=0)([image, augmented_image, augmented_image2])\n",
        "\n",
        "        # SAVE image and augmented ones\n",
        "        save_augmented_image(image,i,label,folder_augmented_0)\n",
        "        save_augmented_image(augmented_image,i,label,folder_augmented_0,transf=\"RandRot\")\n",
        "        save_augmented_image(augmented_image2,i,label,folder_augmented_0,transf=\"RandTrasl\")\n",
        "\n",
        "    if label == 1: #41 Class  2: Moderate Demented\n",
        "       augmented_image_list = [RandomRotation_layer1(image) for _ in range(10)]  #list of  50 tensors\n",
        "       augmented_images_conc1= tf.concat([image] + augmented_image_list, axis=0)\n",
        "\n",
        "       augmented_image2_list = [RandomTranslation_layer1(image) for _ in range(10)]\n",
        "       augmented_images_conc2 = tf.concat([image] + augmented_image2_list, axis=0)\n",
        "       augmented_images_conc2=augmented_images_conc2[128:]\n",
        "\n",
        "\n",
        "       augmented_image3_list = [RandomZoom_layer1(image)  for _ in range(10)]\n",
        "       augmented_images_conc3 = tf.concat([image] + augmented_image3_list, axis=0)\n",
        "       augmented_images_conc3=augmented_images_conc3[128:]\n",
        "\n",
        "       augmented_image4_list = [RandomFlip_layer1(image)  for _ in range(10)]\n",
        "       augmented_images_conc4 = tf.concat([image] + augmented_image4_list, axis=0)\n",
        "       augmented_images_conc4=augmented_images_conc4[128:]\n",
        "\n",
        "       augmented_images_conc=tf.concat([augmented_images_conc1, augmented_images_conc2, augmented_images_conc3,augmented_images_conc4], axis=0)\n",
        "\n",
        "      # SAVE image and augmented ones\n",
        "       save_augmented_image(image,i,label,folder_augmented_1,transf=\"RandRot\"+str(n))\n",
        "       n=1\n",
        "       for im in augmented_image_list:\n",
        "         save_augmented_image(im,i,label,folder_augmented_1,transf=\"RandRot\"+str(n))\n",
        "         n+=1\n",
        "       n=1\n",
        "       for im in augmented_image2_list:\n",
        "         save_augmented_image(im,i,label,folder_augmented_1,transf=\"RandTrasl\"+str(n))\n",
        "         n+=1\n",
        "       n=1\n",
        "       for im in augmented_image3_list:\n",
        "         save_augmented_image(im,i,label,folder_augmented_1,transf=\"RandZoom\"+str(n))\n",
        "         n+=1\n",
        "       n=1\n",
        "       for im in augmented_image4_list:\n",
        "         save_augmented_image(im,i,label,folder_augmented_1,transf=\"RandFlip\"+str(n))\n",
        "         n+=1\n",
        "\n",
        "    if label == 2: #2053 Class 3: Non Demented\n",
        "       augmented_image=image  #rescaled\n",
        "       augmented_images_conc=augmented_image\n",
        "       save_augmented_image(augmented_image,i,label,folder_augmented_2)\n",
        "\n",
        "    if label == 3:  #1433 Class 4: Very Mild Demented.\n",
        "        if (random.random()<0.3) : # random probability condition because we didn't want to duplicate our dataset\n",
        "            augmented_image=RandomRotation_layer1(image)\n",
        "            augmented_images_conc = tf.keras.layers.Concatenate(axis=0)([image,augmented_image])\n",
        "            # SAVE image and augmented ones\n",
        "            save_augmented_image(image,i,label,folder_augmented_3)\n",
        "            save_augmented_image(augmented_image,i,label,folder_augmented_3,transf=\"RandRot\")\n",
        "        else:\n",
        "            augmented_image=image  #rescaled\n",
        "            augmented_images_conc=augmented_image\n",
        "            # SAVE image and augmented ones\n",
        "            save_augmented_image(augmented_image,i,label,folder_augmented_3)\n",
        "\n",
        "    #Target Shape of tensor\n",
        "    target_shape = s #(128 128 3)\n",
        "\n",
        "    # Calculate the number of tensors created for each iteration (image + augmented images)\n",
        "    num_tensors = augmented_images_conc.shape[0] // target_shape[0]\n",
        "\n",
        "    # Reshape the tensor into multiple tensors of shape (128, 128, 3)\n",
        "    augmented_images_resh = tf.split(augmented_images_conc, num_tensors, axis=0)\n",
        "\n",
        "    # show images in the first 20 iterations (only if actually augmented)\n",
        "    if i<=20 and num_tensors!=1:\n",
        "      #augmented_images_conc contains the original image and the augmented ones\n",
        "      fig, axs = plt.subplots(nrows=1, ncols=num_tensors, figsize=(10, 10))\n",
        "      for k in range(num_tensors):\n",
        "        imtoshow=(augmented_images_resh[k]).numpy()\n",
        "        imtoshow = imtoshow.astype(np.uint8)  #necessary to visualize correctly the images\n",
        "        axs[k].imshow(imtoshow, cmap='gray')\n",
        "        if k==0:\n",
        "           axs[k].set_title(f\"Image original\")\n",
        "        else:\n",
        "          axs[k].set_title(f\"Image {k + 1} augmented\")\n",
        "      # Adjust layout for better spacing\n",
        "      plt.tight_layout()\n",
        "      # Show the plot\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    for el in augmented_images_resh:\n",
        "       augmented_images.append(el)\n",
        "    for ss in range(len(augmented_images_resh)):\n",
        "       augmented_labels.append(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O11aB3GGn4We"
      },
      "source": [
        "NEW: display number of samples for each class after data augmentation to compare. We use a different function (count) because augmented_labels is a nd array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YlZJOLnn4Wf"
      },
      "outputs": [],
      "source": [
        "number_of0_after=augmented_labels.count(0)  #Class 1 : 568 Mild Demented-->  1704 Mild Demented\n",
        "number_of1_after=augmented_labels.count(1)  # Class  2: 41 Moderate Demented (64 images)-->1681 Moderate Demented\n",
        "number_of2_after=augmented_labels.count(2)  # Class 3: 2053 Non Demented (3200 images)--> 2053  Non Demented\n",
        "number_of3_after=augmented_labels.count(3)  #Class= 4: 1433 Very Mild Demented (2240 images)--> 1860 Very Mild Demented\n",
        "\n",
        "NUM_IMAGES_POST = []\n",
        "\n",
        "for label in classes:\n",
        "    dir_name_augmented= \"Augmented_TrainDataset/\"+label+\"/\"\n",
        "    num=len([name for name in os.listdir(dir_name_augmented)])\n",
        "    print(\"{} class has {} samples in training set, after data augmentation\".format(label,num))\n",
        "    NUM_IMAGES_POST.append(num)\n",
        "\n",
        "NUM_IMAGES_POST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm_ZzbPsn4Wf"
      },
      "outputs": [],
      "source": [
        "#in order to have the augmented dataset the same format of the original one\n",
        "train_ds_aug = tf.data.Dataset.from_tensor_slices((augmented_images,  augmented_labels))\n",
        "train_ds_aug = train_ds_aug.batch(BATCH_SIZE)\n",
        "#check if the augmented dataset is well formatted\n",
        "print(type(train_ds))\n",
        "print(type(train_ds_aug))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEmBUVxKn4Wf"
      },
      "source": [
        "NEW: We  plot an histogram of classes distribution  before and after data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ew0ST1ukxsO"
      },
      "outputs": [],
      "source": [
        "##### plot class distribution before and after the augmentation #####\n",
        "aug_tensors, aug_labels = images_labels_fromDataset(train_ds_aug)\n",
        "aug_train_labels_name=['Mild_Demented' if el==0 else 'Moderate_Demented' if el==1 else 'Non_Demented'  if el==2 else 'Very_Mild_Demented'   for el in aug_labels]\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(12,5))\n",
        "\n",
        "vis_ratio = False # if we want to plot the num of samples or the ratio of each class\n",
        "vis_title = 'Percentage of samples per class' if vis_ratio else 'Number of samples per class'\n",
        "df_cnt = pd.concat([pd.Series(aug_train_labels_name).value_counts(normalize=vis_ratio), pd.Series(train_labels_name).value_counts(normalize=vis_ratio)], axis=1)\n",
        "df_cnt.plot(kind='bar', ax=axs[0], title=vis_title, rot=45, color = ['green', 'purple'])\n",
        "axs[0].legend(['train after data augmentation', 'train before data augmentation'])\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "vis_ratio = True # if we want to plot the num of samples or the ratio of each class\n",
        "vis_title = 'Percentage of samples per class' if vis_ratio else 'Number of samples per class'\n",
        "df_cnt = pd.concat([pd.Series(aug_train_labels_name).value_counts(normalize=vis_ratio), pd.Series(train_labels_name).value_counts(normalize=vis_ratio)], axis=1)\n",
        "df_cnt.plot(kind='bar', ax=axs[1], title=vis_title, rot=45, color = ['green', 'purple'])\n",
        "axs[1].legend(['train after data augmentation', 'train before data augmentation'])\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "plt.savefig('Training set distribution after augmentation.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG4ZHzk-n4Wg"
      },
      "source": [
        "COMMENTS: data of class \"Mild_Demented\" and \"Moderate_Demented\" were augmented properly but their number of samples doesn't overcome the most numerous class (Non_Demented)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVRPrEOun4Wg"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "Because we are working with categorical and noncontinuous data, we want to convert our model into one-hot encodings. One-hot encodings are a way for the model to understand that we're looking at categorial instead of continuous data. Transforming features so that they'll be more understandable is called feature engineering. Learn more about feature engineering [here](https://developers.google.com/machine-learning/crash-course/representation/feature-engineering).\n",
        "\n",
        "NEW The function from the origina code changes data representation in an \"easier\" way: converts the label into a one-hot encoded tensor (one-hot representation with a depth of NUM_CLASSES) using TensorFlow's tf.one_hot function, and returns the original image along with its one-hot encoded label. Many deep learning models can operate only on numeric variables for the algorIthm efficiency [CITAZIONE 2]. For categorical variables for which no ordinal relationship exists, as these, the integer encoding is not enough. One hot encoding converts categorical data to numbers before fitting the model: a new binary variable (column) is added to the dataset for each unique categorical value in the variable (the presence or absence of a \"1\" in a column indicates the class of the input sample) and the original categorical variable is removed from the dataset ; STRENGTH OF ORIGINAL CODE: this avoids introducing ordinal relationships between labels because they are treated as distinct and unrelated; here this works well since target only has 4 possible values. One more reason to apply this function is that many loss functions, such as categorical cross-entropy [citation 3] expect the target labels to be in one-hot encoded format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi-M6iS5kxsH"
      },
      "outputs": [],
      "source": [
        "# Function for one hot encoding of our data\n",
        "def one_hot_label(image, label):\n",
        "    label = tf.one_hot(label, NUM_CLASSES)\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(one_hot_label, num_parallel_calls=AUTOTUNE)\n",
        "val_ds = val_ds.map(one_hot_label, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "train_ds_OLD = train_ds_OLD.map(one_hot_label, num_parallel_calls=AUTOTUNE)\n",
        "val_ds_OLD = val_ds_OLD.map(one_hot_label, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "train_ds_aug = train_ds_aug.map(one_hot_label, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "#The following lines of code makes calling images from our dataset more efficient.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "train_ds_OLD = train_ds_OLD.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds_OLD = val_ds_OLD.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "train_ds_aug = train_ds_aug.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKbcdruln4Wq"
      },
      "outputs": [],
      "source": [
        "one_hot_tensors, one_hot_labels = images_labels_fromDataset(train_ds_aug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3Q52iA2n4Wq"
      },
      "outputs": [],
      "source": [
        "print(one_hot_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7QSxURMkxsQ"
      },
      "source": [
        "# Build the ML Model\n",
        "\n",
        "We'll be using the same architecture for our model as my [Pneumonia Classification NB](https://www.kaggle.com/amyjang/tensorflow-pneumonia-classification-on-x-rays#4.-Build-the-CNN). Using `tf.keras`, we can easily build up the layers of our CNN.\n",
        "\n",
        "NEW: we decided not to change model internal architecture (unless changing Dropout in later hyperparameters tuning) due to limited expertize and time.\n",
        "Inside the tunable model, we made dropout values of last layers scaled in proportion to \"minimal units\" dropout values, to respect the proportion of the chance of keeping units among layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "428c99RGkxsR"
      },
      "outputs": [],
      "source": [
        "def conv_block(filters):\n",
        "    block = tf.keras.Sequential([\n",
        "        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n",
        "        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPool2D()\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    return block\n",
        "\n",
        "def dense_block(units, dropout_rate): # add regularization at the dense layer (dense layer has are more or less fulliconnected, is important to reguraraze weight in order to avoid overfitting)\n",
        "    block = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(), #change the layer: is not practice use a batch normalization after a relu activation along with a dropout\n",
        "        tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "\n",
        "    return block\n",
        "\n",
        "def conv_block_tuned(filters):\n",
        "    block = tf.keras.Sequential([\n",
        "        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n",
        "        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n",
        "        #tf.keras.layers.BatchNormalization(),\n",
        "        #tf.keras.layers.MaxPool2D()\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    return block\n",
        "\n",
        "def dense_block_tuned(units, dropout_rate, L2): # add regularization at the dense layer (dense layer has are more or less fulliconnected, is important to reguraraze weight in order to avoid overfitting)\n",
        "    block = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(L2)),\n",
        "        #tf.keras.layers.BatchNormalization(), #change the layer: is not practice use a batch normalization after a relu activation along with a dropout\n",
        "        tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "\n",
        "    return block\n",
        "\n",
        "def build_model_original():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=(*IMAGE_SIZE, 1)),  \n",
        "\n",
        "        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'), # 16 layers, 3x3 filter, stride=1 (default)\n",
        "        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPool2D(),\n",
        "\n",
        "        conv_block(32),\n",
        "        conv_block(64),\n",
        "\n",
        "        conv_block(128),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "        conv_block(256),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "        tf.keras.layers.Flatten(), #adapt to 1d array\n",
        "        dense_block(512, 0.7),\n",
        "        dense_block(128, 0.5),\n",
        "        dense_block(64, 0.2),\n",
        "\n",
        "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_model_tunable(L2,drop_out):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=(*IMAGE_SIZE, 3)),\n",
        "\n",
        "        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
        "        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPool2D(),\n",
        "\n",
        "        conv_block_tuned(32),\n",
        "        conv_block_tuned(64),\n",
        "        tf.keras.layers.MaxPool2D(),\n",
        "\n",
        "        conv_block_tuned(128),\n",
        "        tf.keras.layers.Dropout(drop_out),\n",
        "        \n",
        "        conv_block_tuned(256),\n",
        "        tf.keras.layers.MaxPool2D(),\n",
        "        \n",
        "        tf.keras.layers.Dropout(drop_out),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "        dense_block_tuned(512, drop_out*3, L2),  # proportion of dropout between layers mantained\n",
        "        dense_block_tuned(128, drop_out*2, L2),\n",
        "        dense_block_tuned(64, drop_out, L2),\n",
        "\n",
        "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBN_M8D3n4Ws"
      },
      "source": [
        "# Training the Model\n",
        "\n",
        "To more efficiently train our model. We will be using callbacks to adjust our learning rate and to stop our model once it converges.\n",
        "\n",
        "The [learning rate](https://developers.google.com/machine-learning/glossary#learning-rate) is a very important hyperparameter in the model. Having a LR that is too high will prevent the model from converging. Having a LR that is too slow will make the process too long. Stopping our model early is one mechanism that prevents overfitting.\n",
        "\n",
        "NEW: The original code fits the model using properly callbacks ( STRENGTH OF ORIGINAL CODE): saves a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved; keeps the model that has achieved the \"best performance\" so far: #stops training if a monitored metric (in this case, validation loss) does not improve after a certain number of epochs (patience).\n",
        "\n",
        "We  plot different time decays for learning rate, assuming for example 50 epochs, to see how it decreases with epochs. We try then fitting the model with some of these learning decays and some of their parameters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn4unl1TkxsU"
      },
      "outputs": [],
      "source": [
        "### plot different learning decays rates ####\n",
        "def exponential_decay(s,lr0):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0*0.1**(epoch / s)\n",
        "    return exponential_decay_fn\n",
        "\n",
        "lr0 = 0.01  #initial learning rate\n",
        "s = 20\n",
        "\n",
        "epochs=np.linspace(1,50,50)\n",
        "lr_epochs=lr0*0.1**(epochs / 20)\n",
        "lr_epochs2=lr0*0.1**(epochs / 30)\n",
        "lr_epochs3=lr0*0.1**(epochs / 5)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(epochs, lr_epochs,label=\"power decay = 20\")\n",
        "plt.plot(epochs, lr_epochs2,label=\"power decay = 30\")\n",
        "plt.plot(epochs, lr_epochs3,label=\"power decay = 5\")\n",
        "\n",
        "plt.title(\"Learning rate decay\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.legend()\n",
        "plt.xlim(-2, 28)\n",
        "\n",
        "plt.savefig('Learning_Rate_Decay.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47pPzhUfkxsU"
      },
      "source": [
        "Let's fit our model!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ORIGINAL MODEL WITH OLD (the discarded ones) DATA\n",
        "model_name = 'alzheimer_model.h5'\n",
        "EPOCHS = 100\n",
        "\n",
        "#with strategy.scope():\n",
        "model = build_model_original()\n",
        "METRICS = [tf.keras.metrics.AUC(name='auc')]\n",
        "\n",
        "optimizer = Adam(learning_rate=lr0) #to inizialize the value of learning rate\n",
        "# Adam optimizes the model's parameters by adapting the learning rates for each parameter individually, helping improve the efficiency of the training process\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.losses.CategoricalCrossentropy(),\n",
        "    metrics=METRICS\n",
        ")\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay(s,lr0))\n",
        "#learning rate schedules are applied in addition to the adaptive learning rates provided by Adam. Learning rate schedules can be used to further adjust the learning rate during training based on a predefined schedule.\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_name,save_best_only=True)\n",
        "#early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5, restore_best_weights=True)  ## not useful along with the checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_original_OLD = model.fit(        #for each epochs the data are shuffled and a portion is used as valid test\n",
        "    train_ds_OLD,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    validation_data=val_ds_OLD,\n",
        "    callbacks=[checkpoint_cb, lr_scheduler],\n",
        "    verbose = 1, #progression bar\n",
        "    epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN1m25WpkxsV"
      },
      "outputs": [],
      "source": [
        "# ORIGINAL MODEL WITH ORIGINAL DATA\n",
        "model_name = 'Original_model.h5'\n",
        "EPOCHS = 100\n",
        "\n",
        "#with strategy.scope():\n",
        "model = build_model_original()\n",
        "tf.keras.utils.plot_model(model, to_file=\"Original_modelplotted.png\", show_shapes=True) #plot the network\n",
        "METRICS = [tf.keras.metrics.AUC(name='auc')]\n",
        "\n",
        "optimizer = Adam(learning_rate=lr0) #to inizialize the value of learning rate\n",
        "# Adam optimizes the model's parameters by adapting the learning rates for each parameter individually, helping improve the efficiency of the training process\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.losses.CategoricalCrossentropy(),\n",
        "    metrics=METRICS\n",
        ")\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay(s,lr0))\n",
        "#learning rate schedules are applied in addition to the adaptive learning rates provided by Adam. Learning rate schedules can be used to further adjust the learning rate during training based on a predefined schedule.\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_name,save_best_only=True)\n",
        "#early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5, restore_best_weights=True)  ## not useful along with the checkpoints\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualkeras.layered_view(model, legend=True, font=ImageFont.truetype(\"arial.ttf\", 30), to_file='original_model_layer_view.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0vLdpa7o1VE"
      },
      "outputs": [],
      "source": [
        "history_original = model.fit(        #for each epochs the data are shuffled and a portion is used as valid test\n",
        "    train_ds,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[checkpoint_cb, lr_scheduler],\n",
        "    verbose = 1, #progression bar\n",
        "    epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QurvCDabn4Wt"
      },
      "source": [
        "# NEW: MODEL TRAINED WITH AUGMENTED DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj8fwSlKkxsW",
        "outputId": "d4ec7d8c-2415-4a74-9366-c58f8a2336ad"
      },
      "outputs": [],
      "source": [
        "# ORIGINAL MODEL WITH AUGMENTED DATA\n",
        "model_name = 'Original_model_augmented_data.h5'\n",
        "EPOCHS = 100\n",
        "\n",
        "model_aug = build_model_original()\n",
        "#tf.keras.utils.plot_model(model, to_file=\"Original_modelplotted.png\", show_shapes=True) #plot the network\n",
        "METRICS = [tf.keras.metrics.AUC(name='auc')]\n",
        "\n",
        "optimizer = Adam(learning_rate=lr0) #to inizialize the value of learning rate\n",
        "# Adam optimizes the model's parameters by adapting the learning rates for each parameter individually, helping improve the efficiency of the training process\n",
        "model_aug.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.losses.CategoricalCrossentropy(),\n",
        "    metrics=METRICS\n",
        ")\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay(s,lr0))\n",
        "#learning rate schedules are applied in addition to the adaptive learning rates provided by Adam. Learning rate schedules can be used to further adjust the learning rate during training based on a predefined schedule.\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_name,save_best_only=True)\n",
        "#early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5, restore_best_weights=True)  ## not useful along with the checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "history_original_AUG = model_aug.fit(        #for each epochs the data are shuffled and a portion is used as valid test\n",
        "    train_ds_aug,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[checkpoint_cb, lr_scheduler],\n",
        "    verbose = 1, #progression bar\n",
        "    epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RV9FjCNn4Wu"
      },
      "source": [
        "# NEW: HYPERPARAMETER TUNING\n",
        "We try to change the model hyperparameters (drop out, L2 regularization and learnong rate decay s) in order to overcome the problem of overfitting end try to improve the given model. The initial idea was to perform a K-cross-validation but it is too time consuming, so once test and train set were fixed we tried different combinations of hyperparameters and based on the metrics plots  (AUC and loss valid / train) we chose the best one with manual visual evaluation. We decided not to tune optimization tecnique, which was set to be \"Adam\", since this already combines momentum damping  and gradient normalization, so we considered this the most efficient. We also  left Categorical Cross Entropy as loss, since it is one easily differentiable and very common loss function for multiclass classification.\n",
        "\n",
        "The following cell only trains one model because we have already chosen the hyperparametres (dropout=0.1, s=20, L2=1e-3). If you want to train a model for every hyperparametres combination ( to check the selected ones were the best),\n",
        "\n",
        "\n",
        "1. just uncomment following lines:\n",
        "drop_out = [0.1, 0.2, 0.3]   #grid of hyperparameter used to train the model\n",
        "s = [5, 20, 30]\n",
        "L2 = [1e-4, 1e-3, 1e-2]\n",
        "\n",
        "2.  comment:\n",
        "drop_out = [0.1]    #selected hyperparameters\n",
        "s = [20]\n",
        "L2 = [1e-3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4BXQeKTkxsW",
        "outputId": "95bb1199-7b74-4e11-fd06-61295a0cbce3"
      },
      "outputs": [],
      "source": [
        "#Hyperparameter tuning with augmented data\n",
        "'''\n",
        "drop_out = [0.1, 0.2, 0.3]   #grid of hyperparameter used to train the model\n",
        "s = [5, 20, 30]\n",
        "L2 = [1e-4, 1e-3, 1e-2]\n",
        "'''\n",
        "drop_out = 0.2    #selected hyperparameters\n",
        "s = 20\n",
        "L2 = 1e-3\n",
        "\n",
        "EPOCHS = 100\n",
        "model_name = 's_'+str(s)+'_L2_'+str(L2)+'_dropOut_'+str(drop_out)+'.h5'\n",
        "\n",
        "#with strategy.scope():\n",
        "model_tuned = build_model_tunable(L2,drop_out)\n",
        "tf.keras.utils.plot_model(model_tuned, to_file=\"Tuned_modelplotted.png\", show_shapes=True) #plot the network\n",
        "METRICS = [tf.keras.metrics.AUC(name='auc')]\n",
        "\n",
        "optimizer = Adam(learning_rate=lr0) #to inizialize the value of learning rate\n",
        "# Adam optimizes the model's parameters by adapting the learning rates for each parameter individually, helping improve the efficiency of the training process\n",
        "model_tuned.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.losses.CategoricalCrossentropy(),\n",
        "    metrics=METRICS\n",
        ")\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay(s,lr0))\n",
        "#learning rate schedules are applied in addition to the adaptive learning rates provided by Adam. Learning rate schedules can be used to further adjust the learning rate during training based on a predefined schedule.\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_name,save_best_only=True)\n",
        "#early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5, restore_best_weights=True)  ## not useful along with the checkpoints\n",
        "\n",
        "\n",
        "'''\n",
        "hparam = {\n",
        "    'drop_out' : drop_out,\n",
        "    's' : s,\n",
        "    'L2' : L2\n",
        "}\n",
        "\n",
        "hparam_grid = ParameterGrid(hparam)\n",
        "\n",
        "history = dict()\n",
        "\n",
        "for p in hparam_grid:\n",
        "    drop_out = p['drop_out']\n",
        "    s = p['s']\n",
        "    L2 = p['L2']\n",
        "\n",
        "    model_name = 's_'+str(s)+'_L2_'+str(L2)+'_dropOut_'+str(drop_out)+'.h5'\n",
        "    \n",
        "    with strategy.scope():\n",
        "        model_tuned = build_model_tunable(L2,drop_out)\n",
        "        tf.keras.utils.plot_model(model_tuned, to_file=model_name[:-3]+'plotted.png', show_shapes=True) #plot the network\n",
        "        METRICS = [tf.keras.metrics.AUC(name='auc')]\n",
        "\n",
        "        model_tuned.compile(\n",
        "            optimizer='adam',\n",
        "            loss=tf.losses.CategoricalCrossentropy(),\n",
        "            metrics=METRICS\n",
        "        )\n",
        "\n",
        "    exponential_decay_fn = exponential_decay(s)\n",
        "\n",
        "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_name,save_best_only=True)\n",
        "    #early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "    history[model_name] = model_tuned.fit(        #for each epochs the data are shuffled and a portion is used as valid test\n",
        "        train_ds_aug,\n",
        "        batch_size = BATCH_SIZE,\n",
        "        validation_data=val_ds,\n",
        "        callbacks=[checkpoint_cb, lr_scheduler],\n",
        "        verbose = 1, #progression bar\n",
        "        epochs=EPOCHS\n",
        "    )\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualkeras.layered_view(model_tuned, legend=True, font=ImageFont.truetype(\"arial.ttf\", 30), to_file='tuned_model_layer_view.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "history_tuned= model_tuned.fit(        #for each epochs the data are shuffled and a portion is used as valid test\n",
        "    train_ds_aug,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[checkpoint_cb, lr_scheduler],\n",
        "    verbose = 1, #progression bar\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_PTHkXikxsX"
      },
      "source": [
        "# Visualize Model Metrics\n",
        "\n",
        "Let's graph the ROC AUC metric and loss after each epoch for the training and validation data. Although we didn't use a random seed for our notebook, the results may slightly vary, generally the scores for the validataion data is similar, if not better, than the training dataset.\n",
        "\n",
        "NEW: We analyze the metrics also for the new models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwzigbZjkxsX",
        "outputId": "a64f38bf-0c7b-47a5-fc21-822ab29ed627"
      },
      "outputs": [],
      "source": [
        "# Original model with old dataset\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "ax = ax.ravel()\n",
        "\n",
        "for i, met in enumerate(['auc', 'loss']):\n",
        "    ax[i].plot(history_original_OLD.history[met])\n",
        "    ax[i].plot(history_original_OLD.history['val_' + met])\n",
        "    ax[i].set_title('Model {}'.format(met))\n",
        "    ax[i].set_xlabel('epochs')\n",
        "    ax[i].set_ylabel(met)\n",
        "    ax[i].legend(['train', 'val'])\n",
        "\n",
        "plt.suptitle('Performance Metrics of Original Model')\n",
        "plt.savefig('Original_model_OLD.png')\n",
        "\n",
        "# Original model\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "ax = ax.ravel()\n",
        "\n",
        "for i, met in enumerate(['auc', 'loss']):\n",
        "    ax[i].plot(history_original.history[met])\n",
        "    ax[i].plot(history_original.history['val_' + met])\n",
        "    ax[i].set_title('Model {}'.format(met))\n",
        "    ax[i].set_xlabel('epochs')\n",
        "    ax[i].set_ylabel(met)\n",
        "    ax[i].legend(['train', 'val'])\n",
        "\n",
        "plt.suptitle('Performance Metrics of Original Model')\n",
        "plt.savefig('Original_model.png')\n",
        "\n",
        "\n",
        "# Original model AUGMENTED DATA\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "ax = ax.ravel()\n",
        "\n",
        "for i, met in enumerate(['auc', 'loss']):\n",
        "    ax[i].plot(history_original_AUG.history[met])\n",
        "    ax[i].plot(history_original_AUG.history['val_' + met])\n",
        "    ax[i].set_title('Model {}'.format(met))\n",
        "    ax[i].set_xlabel('epochs')\n",
        "    ax[i].set_ylabel(met)\n",
        "    ax[i].legend(['train', 'val'])\n",
        "\n",
        "plt.suptitle('Performance Metrics of Original Model with augmented data')\n",
        "plt.savefig('Original_model_AUG.png')\n",
        "\n",
        "\n",
        "#tunable models with augmanted dataset\n",
        "#for k in history.keys():\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "ax = ax.ravel()\n",
        "for i, met in enumerate(['auc', 'loss']):\n",
        "    ax[i].plot(history_tuned.history[met])\n",
        "    ax[i].plot(history_tuned.history['val_' + met])\n",
        "    ax[i].set_title('Model {}'.format(met))\n",
        "    ax[i].set_xlabel('epochs')\n",
        "    ax[i].set_ylabel(met)\n",
        "    ax[i].legend(['train', 'val'])\n",
        "\n",
        "plt.suptitle('Performance Metrics of Tuned Model')\n",
        "\n",
        "plt.savefig('Tuned_model_AUG.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEGFYFDekxsY"
      },
      "source": [
        "# Evaluate the Model\n",
        "\n",
        "NEW: Although we used the validatation dataset to continually evaluate the model, we also have a separate testing dataset. Let's prepare the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-SrHHu9kxsY",
        "outputId": "49ddfd86-186c-4126-ba29-dbe12a38cce3"
      },
      "outputs": [],
      "source": [
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"Dataset/test\",\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    color_mode='grayscale'\n",
        ")\n",
        "\n",
        "test_ds = test_ds.map(one_hot_label, num_parallel_calls=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds_OLD = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"Alzheimer_s Dataset/test\",\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    color_mode='grayscale'\n",
        ")\n",
        "\n",
        "test_ds_OLD = test_ds_OLD.map(one_hot_label, num_parallel_calls=AUTOTUNE)\n",
        "test_ds_OLD = test_ds_OLD.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofSaz5_Jn4Ww"
      },
      "source": [
        "# NEW: PLOT CONFUSION MATRIX & PRINT METRICS\n",
        "\n",
        "NEW: the original code simply evaluated auc and loss on test set. We also define a function to plot confusion matrix, display test loss and test accuracy , report also other classification metrics [6] for all the models implemented ( recall, f1-score, specifity at sensitivity, Cohen's kappa )."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2b09fBzkxsj"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes, normalize= False, title= 'Confusion Matrix', cmap= plt.cm.Blues):\n",
        "\t#This function plot confusion matrix method from sklearn package.\n",
        "\n",
        "\n",
        "\tplt.figure(figsize= (12, 12))\n",
        "\tplt.imshow(cm, interpolation= 'nearest', cmap= cmap)\n",
        "\tplt.title(title)\n",
        "\tplt.colorbar()\n",
        "\n",
        "\ttick_marks = np.arange(len(classes))\n",
        "\tplt.xticks(tick_marks, classes, rotation= 45)\n",
        "\tplt.yticks(tick_marks, classes)\n",
        "\n",
        "\tif normalize:\n",
        "\t\tcm = cm.astype('float') / cm.sum(axis= 1)[:, np.newaxis]\n",
        "\t\tprint('Normalized Confusion Matrix')\n",
        "\n",
        "\telse:\n",
        "\t\tprint('Confusion Matrix, Without Normalization')\n",
        "\n",
        "\tprint(cm)\n",
        "\n",
        "\tthresh = cm.max() / 2.\n",
        "\tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "\t\tplt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
        "\n",
        "\tplt.tight_layout()\n",
        "\tplt.ylabel('True Label')\n",
        "\tplt.xlabel('Predicted Label')\n",
        "\n",
        "\tplt.savefig(title+'.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBD45BaCn4Wx"
      },
      "source": [
        "NEW: display balanced accuracy\n",
        "\n",
        "We also display balanced accuracy on the test set. Despite leveling classes, our data augmentation still produces a slighlty unbalanced training set: this is why we also check balanced accuracy.\n",
        "If you want to display history metrics for all the hyperparametres tried, just uncomment  the related  loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "3J-D5m14kxsj",
        "outputId": "dfc32eb9-a166-4693-8599-7a681ac2d1a5"
      },
      "outputs": [],
      "source": [
        "tensors_test_OLD, y_true_OLD=images_labels_fromDataset(test_ds_OLD)\n",
        "y_true_OLD = np.argmax(y_true_OLD, axis=1) # to reestablish the classification from the on hot encoding\n",
        "# Original model with OLD dataset\n",
        "model = load_model('alzheimer_model.h5')\n",
        "test_score = model.evaluate(test_ds_OLD)\n",
        "test_pred = model.predict(test_ds_OLD)\n",
        "y_pred = np.argmax(test_pred, axis=1) #prediction to class labels\n",
        "\n",
        "train_score = model.evaluate(train_ds_OLD, verbose= 1)\n",
        "valid_score = model.evaluate(val_ds_OLD, verbose= 1)\n",
        "\n",
        "balanced_acc = balanced_accuracy_score(y_true_OLD, y_pred)\n",
        "\n",
        "print(\"Balanced Accuracy Test Set:\", balanced_acc)\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train AUC: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation AUC: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test AUC: \", test_score[1])\n",
        "\n",
        "cm= confusion_matrix(y_true_OLD, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix Original Model Original dataset')\n",
        "print(classification_report(y_true_OLD, y_pred, target_names= classes,zero_division=1))\n",
        "\n",
        "# ------------------------------\n",
        "tensors_test, y_true=images_labels_fromDataset(test_ds)\n",
        "y_true = np.argmax(y_true, axis=1) # to reestablish the classification from the on hot encoding\n",
        "\n",
        "# Original model\n",
        "model = load_model('Original_model.h5')\n",
        "test_score = model.evaluate(test_ds)\n",
        "test_pred = model.predict(test_ds)\n",
        "y_pred = np.argmax(test_pred, axis=1) #prediction to class labels\n",
        "\n",
        "train_score = model.evaluate(train_ds, verbose= 1)\n",
        "valid_score = model.evaluate(val_ds, verbose= 1)\n",
        "\n",
        "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"Balanced Accuracy Test Set:\", balanced_acc)\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train AUC: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation AUC: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test AUC: \", test_score[1])\n",
        "\n",
        "cm= confusion_matrix(y_true, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix Original Model')\n",
        "print(classification_report(y_true, y_pred, target_names= classes,zero_division=1))\n",
        "\n",
        "\n",
        "# Original model with augmented data\n",
        "model = load_model('Original_model_augmented_data.h5')\n",
        "test_score = model.evaluate(test_ds)\n",
        "test_pred = model.predict(test_ds)\n",
        "y_pred = np.argmax(test_pred, axis=1) #prediction to class labels\n",
        "\n",
        "train_score = model.evaluate(train_ds, verbose= 1)\n",
        "valid_score = model.evaluate(val_ds, verbose= 1)\n",
        "\n",
        "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"Balanced Accuracy Test Set:\", balanced_acc)\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train AUC: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation AUC: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test AUC: \", test_score[1])\n",
        "\n",
        "cm= confusion_matrix(y_true, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix Original Model augmented data')\n",
        "print(classification_report(y_true, y_pred, target_names= classes,zero_division=1))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#  Models with hyperparameters tuning:\n",
        "for k in history.keys():\n",
        "    model = load_model(k)\n",
        "    test_score = model.evaluate(test_ds)\n",
        "    test_pred = model.predict(test_ds)\n",
        "    y_pred = np.argmax(test_pred, axis=1) #prediction to class labels\n",
        "\n",
        "    train_score = model.evaluate(train_ds, verbose= 1)\n",
        "    valid_score = model.evaluate(val_ds, verbose= 1)\n",
        "\n",
        "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "    print(\"Balanced Accuracy Test Set:\", balanced_acc)\n",
        "    print(\"Train Loss: \", train_score[0])\n",
        "    print(\"Train AUC: \", train_score[1])\n",
        "    print('-' * 20)\n",
        "    print(\"Validation Loss: \", valid_score[0])\n",
        "    print(\"Validation AUC: \", valid_score[1])\n",
        "    print('-' * 20)\n",
        "    print(\"Test Loss: \", test_score[0])\n",
        "    print(\"Test AUC: \", test_score[1])\n",
        "\n",
        "    cm= confusion_matrix(y_true, y_pred)\n",
        "    plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix '+k[:-3])\n",
        "    print(classification_report(y_true, y_pred, target_names= classes,zero_division=1))\n",
        "\"\"\"\n",
        "\n",
        "model = load_model('s_20_L2_0.001_dropOut_0.1.h5')\n",
        "test_score = model.evaluate(test_ds)\n",
        "test_pred = model.predict(test_ds)\n",
        "y_pred = np.argmax(test_pred, axis=1) #prediction to class labels\n",
        "\n",
        "train_score = model.evaluate(train_ds, verbose= 1)\n",
        "valid_score = model.evaluate(val_ds, verbose= 1)\n",
        "\n",
        "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"Balanced Accuracy Test Set:\", balanced_acc)\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train AUC: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation AUC: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test AUC: \", test_score[1])\n",
        "\n",
        "cm= confusion_matrix(y_true, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix Original Model augmented data')\n",
        "print(classification_report(y_true, y_pred, target_names= classes,zero_division=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuirylndn4Wy"
      },
      "source": [
        "# NEW: Transfer Learning\n",
        "\n",
        "For dealing with lack of data in biomedical domain, transfer learning is used: pretrained AlexNet [6], Densenet121, Resnet50, Resnet101 [8] and VggNet19 is customised and used in our proposed models ro identify AD stages.\n",
        "We use transfer learning methods to see if the usage of a more complex and pre-trained NN could lead to better performances. We use Desnet121, Resnet50, Resnet101 and Vgg19. We freezed the backbone, resized the input layer, added in order an average maxpooling layer, a dense layer activated by a Relu and the output layer in SoftMax activation.\n",
        "To use the transfer learning nets we had to concatenate the 1d (gray scale) tensor into a 3d tensor in order to simulate the rgb format because the model was made to recognise colored images. [13]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4goJd8jjn4Wz"
      },
      "outputs": [],
      "source": [
        "img_width, img_height = 128, 128 #Our images dimensions\n",
        "#Densenet121, model pretrained with the dataset imagenet, the top layer is removed, and the input shape is equal to our image shape, we have also to make our images RGB images\n",
        "#because the model accepts only this type\n",
        "from keras.layers import Input, Concatenate\n",
        "# load\n",
        "input_image = Input(shape=(img_width, img_height, 1))\n",
        "\n",
        "# Concatenate the same tensor in order to get a 3d tensor suitable for the transfer learing net\n",
        "RGBinput = Concatenate()([input_image, input_image, input_image])\n",
        "\n",
        "# USe DensNet121 with the rigth inputs\n",
        "base_model = DenseNet121(weights='imagenet', include_top=False, input_tensor=RGBinput)\n",
        "\n",
        "#base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "x = base_model.output #Change the output of the pretrained model\n",
        "x = GlobalAveragePooling2D()(x) #Reduce the computational cost\n",
        "x = Dense(1024, activation='relu')(x) #Dense Layer for output\n",
        "predictions = Dense(4, activation='softmax')(x) #Final layer with the probability of each classes for every input\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "for layer in base_model.layers: #Freeze the backbone of the model(because these layers are)\n",
        "    layer.trainable = False\n",
        "optimizer=Adam(learning_rate=0.001)\n",
        "METRICS = [tf.keras.metrics.AUC(name='auc'),'accuracy']\n",
        "model.compile(optimizer=optimizer, loss=tf.losses.CategoricalCrossentropy(), metrics=METRICS)\n",
        "\n",
        "visualkeras.layered_view(model, legend=True, font=ImageFont.truetype(\"arial.ttf\", 30), to_file='DensNet121_layer_view.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR1IN3KPn4Wz",
        "outputId": "ce50e241-2312-4a8e-c65b-51d606fb722b"
      },
      "outputs": [],
      "source": [
        "#Train the model\n",
        "historydensenet101=model.fit(\n",
        "    train_ds_aug,\n",
        "    epochs=50,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1,\n",
        "    validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "vAFIn0rmn4W0",
        "outputId": "76db008d-92a4-47b5-e08c-fdaaa462dcbf"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "ax = ax.ravel()\n",
        "#Let's plot the performance metrics\n",
        "for i, met in enumerate(['auc', 'loss']):\n",
        "    ax[i].plot(historydensenet101.history[met])\n",
        "    ax[i].plot(historydensenet101.history['val_' + met])\n",
        "    ax[i].set_title('Model {}'.format(met))\n",
        "    ax[i].set_xlabel('epochs')\n",
        "    ax[i].set_ylabel(met)\n",
        "    ax[i].legend(['train', 'val'])\n",
        "\n",
        "plt.suptitle('Performance Metrics of DenseNet121 Model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aLEQMbAEn4W0",
        "outputId": "ca6a2040-e9f4-4dc4-fa4f-99d70cd27192"
      },
      "outputs": [],
      "source": [
        "#Validate the model and plot the confusion matrix\n",
        "test_score = model.evaluate(test_ds)\n",
        "test_pred = model.predict(test_ds)\n",
        "y_pred = np.argmax(test_pred, axis=1) #prediction to class labels\n",
        "train_score = model.evaluate(train_ds, verbose= 1)\n",
        "valid_score = model.evaluate(val_ds, verbose= 1)\n",
        "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"Balanced Accuracy Test Set:\", balanced_acc)\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train AUC: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation AUC: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test AUC: \", test_score[1])\n",
        "\n",
        "cm= confusion_matrix(y_true, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix DenseNet121')\n",
        "#print(classification_report(y_true, y_pred, target_names= classes,zero_division=5))\n",
        "\n",
        "#kappa=cohen_kappa(cm)\n",
        "#print(\"Cohen's Kappa \", kappa)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f32Tlb1n4W1",
        "outputId": "78a330c5-ca23-43a9-c152-b4331321e54e"
      },
      "outputs": [],
      "source": [
        "#Transfer Learning with Resnet50\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=RGBinput)\n",
        "#base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "optimizer=Adam(learning_rate=0.001)\n",
        "METRICS = [tf.keras.metrics.AUC(name='auc'),'accuracy']\n",
        "model.compile(optimizer=optimizer, loss=tf.losses.CategoricalCrossentropy(), metrics=METRICS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "t1iB_U1R5aZY",
        "outputId": "984d3bda-2746-429b-8923-6942f2fcd630"
      },
      "outputs": [],
      "source": [
        "visualkeras.layered_view(model, legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91WfTur9n4W1",
        "outputId": "3cd49fe6-d2c7-474c-969d-5e758ab56d90"
      },
      "outputs": [],
      "source": [
        "#Train the model\n",
        "historyresnet50=model.fit(\n",
        "    train_ds_aug,\n",
        "    epochs=50,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1,\n",
        "    validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "wv1Ri-yYn4W1",
        "outputId": "aa3acf01-5a05-4465-b406-a2a2bf8e34e8"
      },
      "outputs": [],
      "source": [
        "#Plot the performance metrics\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "ax = ax.ravel()\n",
        "\n",
        "for i, met in enumerate(['auc', 'loss']):\n",
        "    ax[i].plot(historyresnet50.history[met])\n",
        "    ax[i].plot(historyresnet50.history['val_' + met])\n",
        "    ax[i].set_title('Model {}'.format(met))\n",
        "    ax[i].set_xlabel('epochs')\n",
        "    ax[i].set_ylabel(met)\n",
        "    ax[i].legend(['train', 'val'])\n",
        "\n",
        "plt.suptitle('Performance Metrics of ResNet50 Model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lLdhgUBvn4W2",
        "outputId": "588c859f-13c9-41aa-e279-9a9cb0685b9c"
      },
      "outputs": [],
      "source": [
        "#Evaluate the model\n",
        "test_score = model.evaluate(test_ds)\n",
        "test_pred = model.predict(test_ds)\n",
        "y_pred = np.argmax(test_pred, axis=1) #prediction to class labels\n",
        "train_score = model.evaluate(train_ds, verbose= 1)\n",
        "valid_score = model.evaluate(val_ds, verbose= 1)\n",
        "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"Balanced Accuracy Test Set:\", balanced_acc)\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train AUC: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation AUC: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test AUC: \", test_score[1])\n",
        "\n",
        "\n",
        "cm= confusion_matrix(y_true, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix ResNet50')\n",
        "#print(classification_report(y_true, y_pred, target_names= classes,zero_division=5))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVUGWHS0n4W2",
        "outputId": "6b99b5a7-3754-408d-eaac-d8596d134a50"
      },
      "outputs": [],
      "source": [
        "#Transfer Learning with Resnet101\n",
        "base_model = ResNet101(weights='imagenet', include_top=False, input_tensor=RGBinput)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "optimizer=Adam(learning_rate=0.001)\n",
        "METRICS = [tf.keras.metrics.AUC(name='auc'),'accuracy']\n",
        "model.compile(optimizer=optimizer, loss=tf.losses.CategoricalCrossentropy(), metrics=METRICS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL6S_M8Y2OPS"
      },
      "outputs": [],
      "source": [
        "visualkeras.layered_view(model, legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKnRG__Cn4W3",
        "outputId": "c0be823c-7be9-4bfd-e4ba-a7f2ec122554"
      },
      "outputs": [],
      "source": [
        "#Train model\n",
        "historyresnet101=model.fit(\n",
        "    train_ds_aug,\n",
        "    epochs=25,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1,\n",
        "    validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-aVyuobn4W3",
        "outputId": "60dd591d-21df-4a2d-b3c4-a385a81695c5"
      },
      "outputs": [],
      "source": [
        "#Plot the metrics\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "ax = ax.ravel()\n",
        "\n",
        "for i, met in enumerate(['auc', 'loss']):\n",
        "    ax[i].plot(historyresnet101.history[met])\n",
        "    ax[i].plot(historyresnet101.history['val_' + met])\n",
        "    ax[i].set_title('Model {}'.format(met))\n",
        "    ax[i].set_xlabel('epochs')\n",
        "    ax[i].set_ylabel(met)\n",
        "    ax[i].legend(['train', 'val'])\n",
        "\n",
        "plt.suptitle('Performance Metrics of ResNet101 Model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GyQO9iZn4W3",
        "outputId": "d1e5fb7a-ce75-44e3-a92a-d68e7ee72a6d"
      },
      "outputs": [],
      "source": [
        "#Evaluate tbe model\n",
        "test_score = model.evaluate(test_ds)\n",
        "test_pred = model.predict(test_ds)\n",
        "y_pred = np.argmax(test_pred, axis=1) #prediction to class labels\n",
        "\n",
        "train_score = model.evaluate(train_ds, verbose= 1)\n",
        "valid_score = model.evaluate(val_ds, verbose= 1)\n",
        "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"Balanced Accuracy Test Set:\", balanced_acc)\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train AUC: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation AUC: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test AUC: \", test_score[1])\n",
        "\n",
        "cm= confusion_matrix(y_true, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix ResNet121')\n",
        "#print(classification_report(y_true, y_pred, target_names= classes,zero_division=5))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ywVf94Ln4W4"
      },
      "outputs": [],
      "source": [
        "#Transfer Learning with VGG19\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_tensor=RGBinput)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "optimizer=Adam(learning_rate=0.001)\n",
        "METRICS = [tf.keras.metrics.AUC(name='auc'),'accuracy']\n",
        "model.compile(optimizer=optimizer, loss=tf.losses.CategoricalCrossentropy(), metrics=METRICS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBzvPH-U2kgH"
      },
      "outputs": [],
      "source": [
        "visualkeras.layered_view(model, legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMTLYkRSn4W4",
        "outputId": "f39a60cf-055d-4ae2-b994-495ead73c716"
      },
      "outputs": [],
      "source": [
        "#Train the model\n",
        "historyVGG19=model.fit(\n",
        "    train_ds_aug,\n",
        "    epochs=30,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1,\n",
        "    validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEd8dbQtn4W5",
        "outputId": "5a98a2cc-92c1-4296-c18c-d96f3c036488"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "ax = ax.ravel()\n",
        "#Plot the performance metrics\n",
        "for i, met in enumerate(['auc', 'loss']):\n",
        "    ax[i].plot(historyVGG19.history[met])\n",
        "    ax[i].plot(historyVGG19.history['val_' + met])\n",
        "    ax[i].set_title('Model {}'.format(met))\n",
        "    ax[i].set_xlabel('epochs')\n",
        "    ax[i].set_ylabel(met)\n",
        "    ax[i].legend(['train', 'val'])\n",
        "\n",
        "plt.suptitle('Performance Metrics of VGG19 Model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvN3qZRHn4W5"
      },
      "outputs": [],
      "source": [
        "#Evaluate the model\n",
        "test_score = model.evaluate(test_ds)\n",
        "test_pred = model.predict(test_ds)\n",
        "y_pred = np.argmax(test_pred, axis=1) #prediction to class labels\n",
        "\n",
        "train_score = model.evaluate(train_ds, verbose= 1)\n",
        "valid_score = model.evaluate(val_ds, verbose= 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3cAOG46n4W5",
        "outputId": "f06c72c4-5031-43e7-94e8-a3b8998dbe9b"
      },
      "outputs": [],
      "source": [
        "\n",
        "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"Balanced Accuracy Test Set:\", balanced_acc)\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train AUC: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation AUC: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test AUC: \", test_score[1])\n",
        "\n",
        "cm= confusion_matrix(y_true, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix VGG19')\n",
        "#print(classification_report(y_true, y_pred, target_names= classes,zero_division=1))\n",
        "\n",
        "#kappa=cohen_kappa(cm)\n",
        "#print(\"Cohen's Kappa \", kappa)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE-ZgTawn4W6"
      },
      "source": [
        "# Results:\n",
        "\n",
        " Our model, with the original architecture proposed and without acquiring transfered knowledge, is not able to do  classify the disease stage.\n",
        "\n",
        "The original model with the new dataset struggles to identify moderate demented and mild demented also using augmented data and after hyperarametres tuning. Moderate demented class is the most critical, while Mild Demented shows very little improvement in sensitivities with these precautions. We reach the conclusion that the main problem is the model architecture that is unsuitable.\n",
        "\n",
        "Transfer learning techniques on different models showed higher capabilities in fulfilling the purpose of the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3X80WNnn4W6"
      },
      "source": [
        "# LIMITATIONS:\n",
        "Using the model provided, two classes are not detected even after adjusting the hyperparameters and even after data augmentation.\n",
        "\n",
        "\n",
        "Looking at augmented images, some translation resulted in the region of interest of the image to be cut out: some input useful pixels were thrown away. We couldn't find a proper way to solve this, but translations were performed for really low portions of the dataset, so we thought this could not compromise significantly results,\n",
        "\n",
        "This code is specific for this dataset due to data augmentation that is sized on the strong unbalance of the dataset. So our initial intension of making an adaptable code is not really satisfied.\n",
        "\n",
        "\n",
        "\n",
        "# POSSIBLE IMPROVEMENTS\n",
        "\n",
        "Increase the model complexity (eg. Siamese CNN [5])\n",
        "\n",
        "Try to implement a mixed method that uses deep learning to extract features (low or high level) and an approrpiate machine learning technique that uses these features to classify the patterns: a recent work [1] showed deep feature-based model outperformed  both handcrafted features based  and deep learning methods. Another example of hybrid approach is using a deep learning network  for automatic brain segmentation and then standard classifier (XGBoost shown the best) to classify Alzheimer disease[4].\n",
        "\n",
        "Compare end-to-end deep learning performances with different machine learning methods [7] on the same dataset, to if deep learning  changes results effectively and significantly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIknuVnEn4W6"
      },
      "source": [
        "\n",
        "# REFERENCES\n",
        "[1]Nawaz, H., Maqsood, M., Afzal, S. et al. A deep feature-based real-time sys-tem for Alzheimer disease stage detection. Multimed Tools Appl 80, 35789â€“35807 (2021). https://doi.org/10.1007/s11042-020-09087-y\n",
        "\n",
        "[2]Islam, J., Zhang, Y. Brain MRI analysis for Alzheimerâ€™s disease diagnosis us-ing an ensemble system of deep convolutional neural networks. Brain Inf. 5, 2 (2018). https://doi.org/10.1186/s40708-018-0080-3\n",
        "\n",
        "[3]Kundaram, Swathi S. and Ketki C. Pathak. â€œDeep Learning-Based Alzheimer Dis-ease Detection.â€ Lecture Notes in Electrical Engineering (2020)\n",
        "\n",
        "[4]C.H. Suh, W.H. Shim, S.J. Kim, J.H. Roh, J.-H. Lee, M.-J. Kim, S. Park, W. Jung, J. Sung, G.-H. Jahng, and for the Alzheimerâ€™s Disease Neuroimaging Initia-tive. Development and Validation of a Deep Learning-Based Automatic Brain Seg-mentation and Classification Algoruthm for Alzheimer Disease using 3D T1- Weighted Volumetric Images American Journal of Neuroradiology December 2020, 41 (12) 2227-2234\n",
        "\n",
        "[5]Mehmood A, Maqsood M, Bashir M, Shuyuan Y. A Deep Siamese Convolution Neural Network for Multi-Class Classification of Alzheimer Disease. Brain Sci. 2020 Feb 5;10(2):84. doi: 10.3390/brainsci10020084\n",
        "\n",
        "[6]Ghazal, Taher & Abbas, Sagheer & Munir, Sundus & Khan, Muhammad & Ahmad, Mu-nir & Issa, Ghassan & Zahra, Binish & Hasan, Mohammad Kamrul. (2021). Alzheimer Disease Detection Empowered with Transfer Learning. Computers, Materials and Continua. 70. 5005-5019. 10.32604/cmc.2022.020866\n",
        "\n",
        "[7]A. W. Salehi, P. Baglat, B. B. Sharma, G. Gupta and A. Upadhya, \"A CNN Model: Earlier Diagnosis and Classification of Alzheimer Disease using MRI,\" 2020 In-ternational Conference on Smart Electronics and Communication (ICOSEC), Trichy, India, 2020, pp. 156-161, doi: 10.1109/ICOSEC49089.2020.9215402\n",
        "\n",
        "[8]A. Farooq, S. Anwar, M. Awais and S. Rehman, \"A deep CNN based multi-class classification of Alzheimer's disease using MRI,\" 2017 IEEE International Con-ference on Imaging Systems and Techniques (IST), Beijing, China, 2017, pp. 1-6, doi: 10.1109/IST.2017.8261460\n",
        "\n",
        "[9] Singh, Narotam & D, Patteshwari & Soni, Neha & Kapoor, Amita. (2022). Auto-mated detection of Alzheimer disease using MRI images and deep neural networks- A review. 10.48550/arXiv.2209.11282\n",
        "\n",
        "[10] Nawaz, Ali & Anwar, Syed & Liaqat, Rehan & Iqbal, Javid & Majid, Muhammad. (2021). Deep Convolutional Neural Network based Classification of Alzheimer's Disease using MRI data.\n",
        "\n",
        "[11] Dimitris Agrafiotis, Chapter 9 - Video Error Concealment. Sergios The-odoridis, Rama Chellappa, Academic Press Library in Signal Processing, Elsevier, Volume 5, 2014, Pages 295-321\n",
        "\n",
        "[12] Alzheimerâ€™s Association (2016) 2016 Alzheimerâ€™s disease facts and figures. Alzheimerâ€™s Dement 12(4):459â€“509\n",
        "\n",
        "[13] https://towardsdatascience.com/transfer-learning-on-greyscale-images-how-to-fine-tune-pretrained-models-on-black-and-white-9a5150755c7a\n",
        "(transfer leanrning approach)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
