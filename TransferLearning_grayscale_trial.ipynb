{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seba485/Deep-Learning-project/blob/main/TransferLearning_grayscale_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using pre-trained models related to our dataset (grayscale, mri dataset)"
      ],
      "metadata": {
        "id": "5godvFqUndea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our MRI dataset is greyscale which consists of only one channel. However, most of the pretrained models are trained on RGB image such as ImageNet. Thus, we firstly finetuned our dataset on pretrained models with ImageNet (RGB) by stacking the same channel 3 times. However, the research  ([Pre-training on Grayscale ImageNet Improves\n",
        "Medical Image Classification](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11134/Xie_Pre-training_on_Grayscale_ImageNet_Improves_Medical_Image_Classification_ECCVW_2018_paper.pdf)) shows that finetuning grayscale X-ray image on grayscale pretraining model has higher AUC score than finetuning grayscale image on original RGB pretraining model. (*However, the difference is not significant. AUC 0.7706 and AUC 0.7498) Therefore, we finetuned our MRI dataset using open source models that have been trained on grayscale dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "klwYxSBm2630"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import copy\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    print('running the notebook in colab')\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e0EJchA0LsP",
        "outputId": "bb702fb7-0ceb-4dc7-8fe6-e124d2d36569"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "running the notebook in colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & Prepare Dataset\n",
        "\n",
        "We have to load our dataset as pytorch format in order to use pytorch-based models provided by huggingface. Also, the same train and val set division and the same augmented training set should be used to identically compare the model performance. Therefore, the data type conversion from tensorflow dataloader to pytorch dataloader is required.\n",
        "\n",
        "We first tried to load the already preprocessed tensorflow dataset and change it to pytorch format. However, we encountered various errors, mainly not being able to scale the input image range from [0,255] to [0,1].\n",
        "\n",
        "Thus, we alternatively used saved image files to make pytorch format."
      ],
      "metadata": {
        "id": "PEJsM8YynHjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Failed codes commented out here)\n",
        "<!---\n",
        "\n",
        "```\n",
        "# transform tensorflow dataset to pytorch dataloader\n",
        "\n",
        "############ Idea 1 ############ -> [x]\n",
        "\n",
        "# load saved augmented dataset\n",
        "dataset_path = '/content/drive/MyDrive/UNIPD-DLNR/Dataset'\n",
        "train_ds_aug_tf = tf.data.Dataset.load(dataset_path+'/train_ds_aug.tfrecord')\n",
        "val_ds_tf = tf.data.Dataset.load(dataset_path+'/val_ds.tfrecord')\n",
        "val_ds_tf = tf.data.Dataset.load('/content/drive/MyDrive/UNIPD-DLNR/Dataset/val_ds.tfrecord')\n",
        "train_ds_tf = tf.data.Dataset.load(dataset_path+'/train_ds.tfrecord')\n",
        "\n",
        "# Step 1) extract images, labels from tf dataset\n",
        "def images_labels_fromDataset(dataset):\n",
        "    dataset_it = dataset.as_numpy_iterator() # Numpy iterator, the object in input is not iterable\n",
        "    all_labels=[]; all_images=[]\n",
        "    for batch_images, batch_labels in dataset_it:\n",
        "        all_labels.extend(batch_labels)\n",
        "        # batch_images = batch_images.astype(np.uint8)\n",
        "        all_images.extend(batch_images)\n",
        "    return all_images, all_labels\n",
        "\n",
        "# tensors_train, labels_train = images_labels_fromDataset(train_ds_aug_tf)\n",
        "tensors_val, labels_val = images_labels_fromDataset(val_ds_tf)\n",
        "# tensors_train_, labels_train_ = images_labels_fromDataset(train_ds_tf)\n",
        "\n",
        "# Step 2) list -> torch dataset\n",
        "# use transforms.ToTensor to scale images from [0,255] to [0,1]\n",
        "\n",
        "# print(type(val_ds_tf))\n",
        "# AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "# val_ds_tf = val_ds_tf.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "# print(type(val_ds_tf))\n",
        "\n",
        "basic_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    # transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(), # can be scaled only if the numpy.ndarray has dtype = np.uint8\n",
        "                           # however, type changes to float after torch.Tensor(ndarray)\n",
        "                           # therefore, manually scale it to [0,1]\n",
        "    transforms.Normalize([73.4793],[83.3339]),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "])\n",
        "\n",
        "''' Calculate mean, std of train dataset for normalization\n",
        "def check_img_stats(loader):\n",
        "    mean = 0.\n",
        "    std = 0.\n",
        "    for images, _ in loader:\n",
        "        batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
        "        # images = images.view(batch_samples, images.size(1), -1)\n",
        "        images = images.view(batch_samples, 1, -1)\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std += images.std(2).sum(0)\n",
        "    mean /= len(loader.dataset)\n",
        "    std /= len(loader.dataset)\n",
        "    print(mean, std)\n",
        "\n",
        "check_img_stats(train_ds_aug)\n",
        "# > tensor([73.4793]) tensor([83.3339])\n",
        "\n",
        "check_img_stats(train_ds)\n",
        "# > tensor([70.7964]) tensor([82.4302])\n",
        "'''\n",
        "\n",
        "# train_ds_aug_dataset = TensorDataset(torch.Tensor(tensors_train), torch.Tensor(labels_train))\n",
        "val_ds_dataset = TensorDataset(torch.Tensor(tensors_val), torch.Tensor(labels_val))\n",
        "# train_ds_dataset = TensorDataset(torch.Tensor(tensors_train_), torch.Tensor(labels_train_))\n",
        "# x = torch.Tensor(tensors_train_)\n",
        "# y = torch.Tensor(labels_train_)\n",
        "# train_ds_dataset = TensorDataset(x, y)\n",
        "\n",
        "# train_ds_aug_dataset.transform = basic_transform\n",
        "val_ds_dataset.transform = basic_transform\n",
        "# train_ds_dataset.transform = basic_transform\n",
        "\n",
        "# check if scaling worked -> [x]\n",
        "images, _ = next(iter(val_ds_dataset))\n",
        "pd.Series(images.reshape(-1)).hist() # should be [0,1] range\n",
        "\n",
        "# Step 3) torch dataset -> torch dataloader\n",
        "BATCH_SIZE = 40\n",
        "# train_ds_aug = DataLoader(train_ds_aug_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_ds = DataLoader(val_ds_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "# train_ds = DataLoader(train_ds_dataset, batch_size=BATCH_SIZE, shuffle=True)```\n",
        "\n",
        "\n",
        "############ Idea 2 ############ -> [x]\n",
        "class CustomPyTorchDataset(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "\n",
        "        image = sample['image']\n",
        "        label = sample['label']\n",
        "        # print(image.shape)\n",
        "        # image = np.squeeze(image, axis=0)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {'image': image, 'label': label}\n",
        "\n",
        "torch_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "])\n",
        "BATCH_SIZE = 40\n",
        "\n",
        "# train set\n",
        "train_ds_aug_np = [{'image': img.numpy(), 'label': l.numpy()} for img, l in train_ds_aug_tf]\n",
        "train_ds_aug_dataset = CustomPyTorchDataset(dataset=train_ds_aug_np, transform=torch_transform)\n",
        "train_ds_aug = DataLoader(train_ds_aug_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# val set\n",
        "val_ds_np = [{'image': img.numpy(), 'label': l.numpy()} for img, l in val_ds_tf]\n",
        "val_ds_dataset = CustomPyTorchDataset(dataset=val_ds_np, transform=torch_transform)\n",
        "val_ds = DataLoader(val_ds_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "```\n",
        "\n",
        "-->\n"
      ],
      "metadata": {
        "id": "PjrLvC_sKJge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save validation arrays to image files\n",
        "'''\n",
        "val_ds_tf = tf.data.Dataset.load('/content/drive/MyDrive/UNIPD-DLNR/Dataset/val_ds.tfrecord')\n",
        "class_name = ['Non_Demented', 'Mild_Demented', 'Very_Mild_Demented', 'Moderate_Demented']\n",
        "folder_path = '/content/drive/MyDrive/UNIPD-DLNR/Dataset/validation'\n",
        "\n",
        "from tqdm import tqdm\n",
        "import imageio\n",
        "\n",
        "for i in tqdm(range(len(tensors_val))):\n",
        "    img = np.squeeze(tensors_val[i], axis=-1).astype(np.uint8)\n",
        "    path = os.path.join(folder_path, class_name[labels_val[i]], f\"{i}.jpg\")\n",
        "    imageio.imwrite(path, img)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3P1alyhFmmA",
        "outputId": "5008fdd9-39e8-425f-b396-cf01db01e8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1023/1023 [00:04<00:00, 217.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. [Theem/fasterrcnn_resnet50_fpn_grayscale](https://huggingface.co/Theem/fasterrcnn_resnet50_fpn_grayscale)\n",
        ": PyTorch FasterRCNN with ResNet50 backbone finetuned on grayscale COCO\n",
        "- COCO Dataset: The COCO (Common Objects in Context) dataset is a large-scale image recognition dataset for object detection, segmentation, and captioning tasks. It contains over 330,000 images, each annotated with 80 object categories and 5 captions describing the scene.\n",
        "\n",
        "***Training Failed**\n",
        ":the model is specifically designed for object detection task while our task is image classification. Even though we modified the last layers into classifier layers, the model training was not successful as the model required the target label for object detection."
      ],
      "metadata": {
        "id": "viAWgKn7KdQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########### LOAD DATASET ###########\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/UNIPD-DLNR/Dataset'\n",
        "BATCH_SIZE = 40\n",
        "\n",
        "def check_img_stats(loader):\n",
        "    mean = 0.\n",
        "    std = 0.\n",
        "    for images, _ in tqdm(loader):\n",
        "        batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
        "        images = images.view(batch_samples, images.size(1), -1)\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std += images.std(2).sum(0)\n",
        "    mean /= len(loader.dataset)\n",
        "    std /= len(loader.dataset)\n",
        "    print(mean, std)\n",
        "    return mean, std\n",
        "\n",
        "# load train dataset\n",
        "train_path = dataset_path+'/Augmented_TrainDataset'\n",
        "'''\n",
        "basic_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "])\n",
        "train_dataset = datasets.ImageFolder(train_path, basic_transforms)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print('Train dataset statistics:')\n",
        "mean, std = check_img_stats(train_dataloader) # 0.2887, 0.3272\n",
        "'''\n",
        "# normalize it with the updated stats\n",
        "updated_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Normalize([0.2887],[0.3272])\n",
        "])\n",
        "train_dataset = datasets.ImageFolder(train_path, updated_transforms)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# print('Final train dataset statistics:')\n",
        "# check_img_stats(train_dataloader) # almost 0, 1 -> correct\n",
        "\n",
        "# load val dataset\n",
        "val_path = dataset_path+'/validation'\n",
        "val_dataset = datasets.ImageFolder(val_path, updated_transforms)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "# check_img_stats(val_dataloader) # ? , 0.9xxx -> ok\n",
        "\n",
        "# load test dataset\n",
        "test_path = dataset_path+'/test'\n",
        "test_dataset = datasets.ImageFolder(test_path, updated_transforms)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "# check_img_stats(test_dataloader) # -0.0331, 0.9888 -> ok\n"
      ],
      "metadata": {
        "id": "oR5CCdPmOs_Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### LOAD MODEL ###########\n",
        "\n",
        "# Load torchvision model structure & modify it for grescale images\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
        "model.backbone.body.conv1 = torch.nn.Conv2d(1, 64,\n",
        "                            kernel_size=(7, 7), stride=(2, 2),\n",
        "                            padding=(3, 3), bias=False).requires_grad_(True)\n",
        "\n",
        "# Load pretrained weights to the model\n",
        "model_path = '/content/drive/MyDrive/UNIPD-DLNR/pretrained_model/fasterrcnn_resnet50_fpn_coco_grayscale.pth'\n",
        "state_dict = torch.load(model_path)['model']\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Romove object detecion box predictor & Add classifier layer\n",
        "num_classes = 4\n",
        "model.roi_heads.box_predictor = nn.Sequential(\n",
        "    nn.Linear(1024,256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, num_classes)\n",
        ")\n",
        "\n",
        "# Freeze model params except for the classifier layer\n",
        "only_last = True\n",
        "if only_last:\n",
        "    for param in model.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.rpn.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.roi_heads.box_head.parameters():\n",
        "        param.requires_grad = False\n",
        "else:\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True"
      ],
      "metadata": {
        "id": "HQgOYv0qtH17"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### FINETUNE ###########\n",
        "\n",
        "# Set training hyperparameters\n",
        "epochs = 20\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # can be used for multi-class classification, not one-hot encoded targets\n",
        "\n",
        "params_to_update = []\n",
        "for name,param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        print(\"\\t\",name)\n",
        "        params_to_update.append(param)\n",
        "optimizer = torch.optim.AdamW(params_to_update, lr=0.001, weight_decay=0.01)\n",
        "\n",
        "# Load model to GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)\n",
        "\n",
        "dataloaders = {'train':train_dataloader, 'val':val_dataloader}"
      ],
      "metadata": {
        "id": "SwMkUnCuqzWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "\n",
        "since = time.time()\n",
        "\n",
        "val_acc_history = []\n",
        "\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                # Get model outputs and calculate loss\n",
        "                # Special case for inception because in training it has an auxiliary output. In train\n",
        "                #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                #   but in testing we only consider the final output.\n",
        "\n",
        "                targets = {'labels':labels}\n",
        "\n",
        "                outputs = model(inputs, targets)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "        epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "        # deep copy the model\n",
        "        if phase == 'val' and epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        if phase == 'val':\n",
        "            val_acc_history.append(epoch_acc)\n",
        "\n",
        "    print()\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "# save model\n",
        "model_name = f\"fasterrcnn_coco_greyscale_e{epochs}.pt\"\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/UNIPD-DLNR/model' + model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "Kpo_tvBMgCdt",
        "outputId": "81b5244f-c57e-480f-d6f6-e3e82879c52b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "string indices must be integers",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-ea576031dd7a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                         torch._assert(\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### raedinkhaled/vit-base-mri\n",
        ": fine-tuned version of google/vit-base-patch16-224-in21k on the mriDataSet dataset: https://huggingface.co/raedinkhaled/vit-base-mri\n",
        "\n",
        "While we have to duplicate the single gray-scale channel three times to be able to use this model, we thought it's worth trying as the model is fine-tuned on mri dataset.\n",
        "\n",
        "However, there were several unexpected errors occured during training, so we decided to discard this method."
      ],
      "metadata": {
        "id": "LNwotgaxrIOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########### LOAD DATASET ###########\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/UNIPD-DLNR/Dataset'\n",
        "BATCH_SIZE = 40\n",
        "\n",
        "def check_img_stats(loader):\n",
        "    mean = 0.\n",
        "    std = 0.\n",
        "    for images, _ in tqdm(loader):\n",
        "        batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
        "        images = images.view(batch_samples, images.size(1), -1)\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std += images.std(2).sum(0)\n",
        "    mean /= len(loader.dataset)\n",
        "    std /= len(loader.dataset)\n",
        "    print(mean, std)\n",
        "    return mean, std\n",
        "\n",
        "# load train dataset\n",
        "train_path = dataset_path+'/Augmented_TrainDataset'\n",
        "'''\n",
        "basic_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "])\n",
        "train_dataset = datasets.ImageFolder(train_path, basic_transforms)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print('Train dataset statistics:')\n",
        "mean, std = check_img_stats(train_dataloader) # 0.2887, 0.3272\n",
        "'''\n",
        "# normalize it with the updated stats\n",
        "updated_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    # transforms.Normalize([0.2887, 0.2887, 0.2887],[0.3272, 0.3272, 0.3272])\n",
        "])\n",
        "train_dataset = datasets.ImageFolder(train_path, updated_transforms)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# print('Final train dataset statistics:')\n",
        "# check_img_stats(train_dataloader) # almost 0, 1 -> correct\n",
        "\n",
        "# load val dataset\n",
        "val_path = dataset_path+'/validation'\n",
        "val_dataset = datasets.ImageFolder(val_path, updated_transforms)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "# check_img_stats(val_dataloader) # ? , 0.9xxx -> ok\n",
        "\n",
        "# load test dataset\n",
        "test_path = dataset_path+'/test'\n",
        "test_dataset = datasets.ImageFolder(test_path, updated_transforms)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "# check_img_stats(test_dataloader) # -0.0331, 0.9888 -> ok\n",
        "\n",
        "dataloaders = {'train':train_dataloader, 'val':val_dataloader}"
      ],
      "metadata": {
        "id": "penojteivTEG"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tuned version of google/vit-base-patch16-224-in21k on the mriDataSet dataset: https://huggingface.co/raedinkhaled/vit-base-mri\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "processor = AutoImageProcessor.from_pretrained(\"raedinkhaled/vit-base-mri\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"raedinkhaled/vit-base-mri\")\n",
        "\n",
        "model.classifier = nn.Linear(in_features=768, out_features=4)\n",
        "for param in model.vit.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Set training hyperparameters\n",
        "criterion = nn.CrossEntropyLoss() # can be used for multi-class classification, not one-hot encoded targets\n",
        "\n",
        "params_to_update = []\n",
        "for name,param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        print(\"\\t\",name)\n",
        "        params_to_update.append(param)\n",
        "optimizer = torch.optim.AdamW(params_to_update, lr=0.001, weight_decay=0.01)\n",
        "\n",
        "# Load model to GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH0kXB7aoYRX",
        "outputId": "bc73f318-6183-4909-95f6-75d4488b6efe"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t classifier.weight\n",
            "\t classifier.bias\n",
            "cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTForImageClassification(\n",
              "  (vit): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTAttention(\n",
              "            (attention): ViTSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "since = time.time()\n",
        "\n",
        "val_acc_history = []\n",
        "\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = processor(inputs, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                # Get model outputs and calculate loss\n",
        "                # Special case for inception because in training it has an auxiliary output. In train\n",
        "                #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                #   but in testing we only consider the final output.\n",
        "\n",
        "                # targets = {'labels':labels}\n",
        "\n",
        "                outputs = model(inputs, labels)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "        epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "        # deep copy the model\n",
        "        if phase == 'val' and epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        if phase == 'val':\n",
        "            val_acc_history.append(epoch_acc)\n",
        "\n",
        "    print()\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "bEeCZHMroZXP",
        "outputId": "bd1bdcf2-5cb3-4610-8617-3e70c35c849a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/feature_extraction_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'dtype'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-832f8aec8be8>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# targets = {'labels':labels}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         outputs = self.vit(\n\u001b[0m\u001b[1;32m    795\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mexpected_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/feature_extraction_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### greyscale imagenet\n",
        "- https://github.com/DaveRichmond-/grayscale-imagenet/tree/master\n",
        "- (+additionally finetuned on X-RAY dataset ?)\n",
        "\n",
        "This is the model which is described on the paper above. We expected to use this model but failed to load the model that is provided on the github repository with no description on its usage."
      ],
      "metadata": {
        "id": "Fn97peRfr2Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = '/content/drive/MyDrive/UNIPD-DLNR/pretrained_model/imagenet-gray/model.ckpt-1495066.data-00000-of-00001'\n",
        "path2 = '/content/drive/MyDrive/UNIPD-DLNR/pretrained_model/imagenet-gray/model.ckpt-1495066.index'\n",
        "path3 = '/content/drive/MyDrive/UNIPD-DLNR/pretrained_model/imagenet-gray/model.ckpt-1495066.meta'\n",
        "# model_1 = tf.keras.models.load_model(path3)\n",
        "with tf.Session() as sess:\n",
        "    # Restore the saved model\n",
        "    saver = tf.train.import_meta_graph(path1)\n",
        "    saver.restore(sess, path1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "ypCNY39dr1l6",
        "outputId": "734f3e02-73ec-419c-8525-ff7cbeec9424"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow' has no attribute 'Session'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-4a9f8528f878>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/UNIPD-DLNR/pretrained_model/imagenet-gray/model.ckpt-1495066.meta'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# model_1 = tf.keras.models.load_model(path3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Restore the saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
          ]
        }
      ]
    }
  ]
}